{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiEZ4OQ6qEsE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSBcP9_aqEsI"
   },
   "source": [
    "# Load data, Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSpxKbGTqEsJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URI</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50034</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Mauno_J%C3%A4rvel...</td>\n",
       "      <td>Mauno J%C3%A4rvel%C3%A4</td>\n",
       "      <td>mauno jrvel born 25 november 1949 in kaustinen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39362</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/David_W._Jourdan&gt;</td>\n",
       "      <td>David W. Jourdan</td>\n",
       "      <td>david walter jourdan born december 5 1954 is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20786</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Patrick_Roach&gt;</td>\n",
       "      <td>Patrick Roach</td>\n",
       "      <td>patrick roach born march 4 1969 is a canadian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26367</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Louis_Sauer&gt;</td>\n",
       "      <td>Louis Sauer</td>\n",
       "      <td>louis lou sauer aka louis edward sauer born 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14855</th>\n",
       "      <td>&lt;http://dbpedia.org/resource/Marty_Keough&gt;</td>\n",
       "      <td>Marty Keough</td>\n",
       "      <td>richard martin keough born april 14 1934 in oa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     URI  \\\n",
       "50034  <http://dbpedia.org/resource/Mauno_J%C3%A4rvel...   \n",
       "39362     <http://dbpedia.org/resource/David_W._Jourdan>   \n",
       "20786        <http://dbpedia.org/resource/Patrick_Roach>   \n",
       "26367          <http://dbpedia.org/resource/Louis_Sauer>   \n",
       "14855         <http://dbpedia.org/resource/Marty_Keough>   \n",
       "\n",
       "                          name  \\\n",
       "50034  Mauno J%C3%A4rvel%C3%A4   \n",
       "39362         David W. Jourdan   \n",
       "20786            Patrick Roach   \n",
       "26367              Louis Sauer   \n",
       "14855             Marty Keough   \n",
       "\n",
       "                                                    text  \n",
       "50034  mauno jrvel born 25 november 1949 in kaustinen...  \n",
       "39362  david walter jourdan born december 5 1954 is a...  \n",
       "20786  patrick roach born march 4 1969 is a canadian ...  \n",
       "26367  louis lou sauer aka louis edward sauer born 19...  \n",
       "14855  richard martin keough born april 14 1934 in oa...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki = pd.read_csv('people_wiki.csv')\n",
    "wiki = wiki.sample(frac=0.1, random_state=0) # Using 10% of the data as to reduce compute time\n",
    "wiki.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gsmRKsqEsN"
   },
   "source": [
    "To work with text data, we must first convert the documents into numerical features. Let's extract TF-IDF features for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5mxiLcJqEsO"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.95)  # ignore words with very high doc frequency\n",
    "tf_idf = vectorizer.fit_transform(wiki['text'])\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYyRTa0rqEsQ"
   },
   "source": [
    "Since most documents don't contain every word, many of the TF-IDF entries will be 0. Representing the TF-IDF matrix as a `numpy` matrix will require a lot of unnecessary storage to keep track of all those 0. SciPy provides the idea of a \"sparse matrix\" that only represents the non-zero entries of a matrix to save space. Externally, you treat it just like a numpy `matrix` but it takes up less storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pW25G1_LqEsR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5907, 112801)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = csr_matrix(tf_idf)\n",
    "\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEbH9bvPqEsT"
   },
   "source": [
    "The above matrix contains a TF-IDF score for each of the 5907 pages in the data set and each of the 112801 unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-LgP2iRqEsU"
   },
   "source": [
    "# Normalize all vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjycwCRAqEsU"
   },
   "source": [
    "The k-means algorithm does not directly work with cosine distance, so we take an alternative route to remove length information: we normalize all vectors to be unit length. It turns out that Euclidean distance closely mimics cosine distance when all vectors are unit length. In particular, the squared Euclidean distance between any two vectors of length one is directly proportional to their cosine distance.\n",
    "\n",
    "We can prove this as follows. Let $\\mathbf{x}$ and $\\mathbf{y}$ be normalized vectors, i.e. unit vectors, so that $\\|\\mathbf{x}\\|=\\|\\mathbf{y}\\|=1$. Write the squared Euclidean distance as the dot product of $(\\mathbf{x} - \\mathbf{y})$ to itself:\n",
    "\\begin{align*}\n",
    "\\|\\mathbf{x} - \\mathbf{y}\\|_2^2 &= (\\mathbf{x} - \\mathbf{y})^T(\\mathbf{x} - \\mathbf{y}) & \\text{(def of L2 norm)}\\\\\n",
    "                              &= (\\mathbf{x}^T \\mathbf{x}) - 2(\\mathbf{x}^T \\mathbf{y}) + (\\mathbf{y}^T \\mathbf{y}) & \\text{(FOIL expression)}\\\\\n",
    "                              &= \\|\\mathbf{x}\\|_2^2 - 2(\\mathbf{x}^T \\mathbf{y}) + \\|\\mathbf{y}\\|_2^2 & \\text{(def of L2 norm)}\\\\\n",
    "                              &= 2 - 2(\\mathbf{x}^T \\mathbf{y}) & \\text{($\\mathbf{x}$ and $\\mathbf{y}$ are length 1)}\\\\\n",
    "                              &= 2(1 - (\\mathbf{x}^T \\mathbf{y}))\\\\\n",
    "                              &= 2\\left(1 - \\frac{\\mathbf{x}^T \\mathbf{y}}{\\|\\mathbf{x}\\|_2\\|\\mathbf{y}\\|_2}\\right) & \\text{(Dividing by 1 doesn't change value)}\\\\\n",
    "                              &= 2\\left[\\text{cosine distance}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "This tells us that two **unit vectors** that are close in Euclidean distance are also close in cosine distance. Thus, the k-means algorithm (which naturally uses Euclidean distances) on normalized vectors will produce the same results as clustering using cosine distance as a distance metric.\n",
    "\n",
    "\n",
    "---\n",
    "We import the [`normalize()` function](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) from scikit-learn to normalize all vectors to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQKNfIcCqEsV"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7U_xrTwqEsX"
   },
   "source": [
    "# Implement k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEfCjgiaqEsY"
   },
   "source": [
    "Let us implement the k-means algorithm. First, we choose an initial set of centroids. A common practice is to choose randomly from the data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKSBAVTNqEsZ"
   },
   "outputs": [],
   "source": [
    "def get_initial_centroids(data, k, seed=None):\n",
    "    \"\"\"\n",
    "    Randomly choose k data points as initial centroids\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    n = data.shape[0] \n",
    "        \n",
    "    # Pick K indices from range [0, N).\n",
    "    rand_indices = np.random.randint(0, n, k)\n",
    "    \n",
    "    # Keep centroids as dense format, as many entries will be nonzero due to averaging.\n",
    "    # As long as at least one document in a cluster contains a word,\n",
    "    # it will carry a nonzero weight in the TF-IDF vector of the centroid.\n",
    "    centroids = data[rand_indices,:].toarray()\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2v1FxFiXqEsb"
   },
   "source": [
    "### k-means Algorithm\n",
    "After initialization, the k-means algorithm iterates between the following two steps:\n",
    "1. Assign each data point to the closest centroid. $$z_i \\gets \\mathrm{argmin}_j \\|\\mathbf{\\mu}_j - \\mathbf{x}_i\\|^2$$\n",
    "2. Revise centroids as the mean of the assigned data points. $$\\mathbf{\\mu}_j \\gets \\frac{1}{n_j}\\sum_{i:z_i=j} \\mathbf{x}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZV9NpZa5qEsc"
   },
   "source": [
    "In pseudocode, we iteratively do the following:\n",
    "```python\n",
    "cluster_assignment = assign_clusters(data, centroids)\n",
    "centroids = revise_centroids(data, k, cluster_assignment)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODoiN4ALqEsd"
   },
   "source": [
    "## Assigning clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yT4A-Uu0qEse"
   },
   "source": [
    "How do we implement Step 1 of the main k-means loop above? First we import `pairwise_distances` function from scikit-learn, which calculates Euclidean distances between rows of given arrays. See [this documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.paired_distances.html) for more information.\n",
    "\n",
    "For the sake of demonstration, let's look at documents 100 through 102 as query documents and compute the distances between each of these documents and every other document in the corpus. In the k-means algorithm, we will have to compute pairwise distances between the set of centroids and the set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETeFLowZqEsf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.39996239 1.39958932]\n",
      " [1.40386156 1.39754968]\n",
      " [1.38421176 1.39682604]\n",
      " ...\n",
      " [1.40562888 1.39024794]\n",
      " [1.39673862 1.38306708]\n",
      " [1.40872806 1.40250208]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Get the TF-IDF vectors for documents 100 through 102.\n",
    "queries = tf_idf[100:102,:]\n",
    "\n",
    "# Compute pairwise distances from every data point to each query vector.\n",
    "dist = pairwise_distances(tf_idf, queries, metric='euclidean')\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHodIMqRqEsh"
   },
   "source": [
    "More formally, `dist[i,j]` is assigned the distance between the `i`th row of `X` (i.e., `X[i,:]`) and the `j`th row of `Y` (i.e., `Y[j,:]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oteWmRE3qEsj"
   },
   "outputs": [],
   "source": [
    "c = tf_idf[0:3,:]\n",
    "distances = pairwise_distances(tf_idf, c, metric='euclidean')\n",
    "dist = distances[430,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yda0BuWfqEsv"
   },
   "outputs": [],
   "source": [
    "centroids = tf_idf[0:3,:]\n",
    "d = pairwise_distances(tf_idf, centroids, metric='euclidean')\n",
    "cluster_assignment = np.argmin(d, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FeXVNuXXqEsz"
   },
   "outputs": [],
   "source": [
    "def assign_clusters(data, centroids):\n",
    "    \"\"\"\n",
    "    Parameters:  \n",
    "      - data      - is an np.array of float values of length N.  \n",
    "      - centroids - is an np.array of float values of length k.\n",
    "\n",
    "    Returns  \n",
    "      -  A np.array of length N where the ith index represents which centroid \n",
    "         data[i] was assigned to. The assignments range between the values 0, ..., k-1.\n",
    "    \"\"\"\n",
    "    distances = pairwise_distances(data, centroids, metric='euclidean')\n",
    "    cluster_assignments = np.argmin(distances, axis=1)\n",
    "    return cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BYZc94XqqEs8"
   },
   "source": [
    "## Revising clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIzsCK3wqEs9"
   },
   "source": [
    "Let's turn to Step 2 of the k-means algorithm, where we compute the new centroids given the cluster assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lsrrqXiqEs9"
   },
   "source": [
    "SciPy and NumPy arrays allow for filtering via Boolean masks. For instance, we filter all data points that are assigned to cluster 0 by writing\n",
    "```python\n",
    "data[cluster_assignment==0,:]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VnTum_RqEtQ"
   },
   "outputs": [],
   "source": [
    "def revise_centroids(data, k, cluster_assignment):\n",
    "    \"\"\"\n",
    "    Parameters:  \n",
    "      - data               - is an np.array of float values of length N.\n",
    "      - k                  - number of centroids\n",
    "      - cluster_assignment - np.array of length N where the ith index represents which \n",
    "                             centroid data[i] was assigned to. The assignments range between the values 0, ..., k-1.\n",
    "\n",
    "    Returns  \n",
    "      -  A np.array of length k for the new centroids.\n",
    "    \"\"\"\n",
    "    new_centroids = []\n",
    "    for i in range(k):\n",
    "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
    "        member_data_points = data[cluster_assignment == i]\n",
    "        # Compute the mean of the data points. Fill in the blank (RHS only)\n",
    "        centroid = member_data_points.mean(axis=0)\n",
    "        \n",
    "        # Convert numpy.matrix type to numpy.ndarray type\n",
    "        centroid = centroid.A1\n",
    "        new_centroids.append(centroid)\n",
    "        \n",
    "    new_centroids = np.array(new_centroids)\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iySadBTFqEtU"
   },
   "source": [
    "### Assessing convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26Op9TPuqEtV"
   },
   "source": [
    "How can we tell if the k-means algorithm is converging? We can look at the cluster assignments and see if they stabilize over time. In fact, we'll be running the algorithm until the cluster assignments stop changing at all. To be extra safe, and to assess the clustering performance, we'll be looking at an additional criteria: the sum of all squared distances between data points and centroids. This is defined as\n",
    "$$\n",
    "J(\\mathcal{Z},\\mu) = \\sum_{j=0}^{k-1} \\sum_{i=1:z_i = j}^n \\|\\mathbf{x}_i - \\mu_j\\|^2.\n",
    "$$\n",
    "The smaller the distances, the more homogeneous the clusters are. In other words, we'd like to have \"tight\" clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwD2YztbqEtV"
   },
   "outputs": [],
   "source": [
    "def compute_heterogeneity(data, k, centroids, cluster_assignment):\n",
    "    \"\"\"\n",
    "    Computes the heterogeneity metric of the data using the given centroids and cluster assignments.\n",
    "    \"\"\"\n",
    "    heterogeneity = 0.0\n",
    "    for i in range(k):\n",
    "        \n",
    "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
    "        member_data_points = data[cluster_assignment==i,:]\n",
    "        \n",
    "        if member_data_points.shape[0] > 0: # check if i-th cluster is non-empty\n",
    "            # Compute distances from centroid to data point\n",
    "            distances = pairwise_distances(member_data_points, [centroids[i]], metric='euclidean')\n",
    "            squared_distances = distances**2\n",
    "            heterogeneity += np.sum(squared_distances)\n",
    "        \n",
    "    return heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOmGx-EHqEtX"
   },
   "source": [
    "Let's compute the cluster heterogeneity for the 2-cluster example we've been considering based on our current cluster assignments and centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5DkruonqEtX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.25"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_heterogeneity(data, 2, centroids, cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpHq8_fkqEtZ"
   },
   "source": [
    "### Combining into a single function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0kAOad1qEtZ"
   },
   "source": [
    "Once the two k-means steps have been implemented, as well as our heterogeneity metric we wish to monitor, it is only a matter of putting these functions together to write a k-means algorithm that\n",
    "\n",
    "* Repeatedly performs Steps 1 and 2\n",
    "* Tracks convergence metrics\n",
    "* Stops if either no assignment changed or we reach a certain number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QveqoxjJqEta"
   },
   "outputs": [],
   "source": [
    "def kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False):\n",
    "    \"\"\"\n",
    "    This function runs k-means on given data and initial set of centroids.\n",
    "    \n",
    "    Parameters:  \n",
    "      - data                 - is an np.array of float values of length N.\n",
    "      - k                    - number of centroids\n",
    "      - initial_centroids    - is an np.array of float values of length k.\n",
    "      - maxiter              - maximum number of iterations to run the algorithm\n",
    "      - record_heterogeneity - if provided an empty list, it will compute the heterogeneity \n",
    "                               at each iteration and append it to the list. \n",
    "                               Defaults to None and won't record heterogeneity.\n",
    "      - verbose              - set to True to display progress. Defaults to False and won't \n",
    "                               display progress.\n",
    "\n",
    "    Returns  \n",
    "      - centroids - A np.array of length k for the centroids upon termination of the algorithm.\n",
    "      - cluster_assignment - A np.array of length N where the ith index represents which \n",
    "                             centroid data[i] was assigned to. The assignments range between the \n",
    "                             values 0, ..., k-1 upon termination of the algorithm.\n",
    "    \"\"\"\n",
    "    centroids = initial_centroids[:]\n",
    "    prev_cluster_assignment = None\n",
    "    \n",
    "    for itr in range(maxiter):  \n",
    "        # Print itereation number\n",
    "        if verbose:\n",
    "            print(itr)\n",
    "        \n",
    "        # 1. Make cluster assignments using nearest centroids\n",
    "        cluster_assignment = assign_clusters(data, initial_centroids)\n",
    "            \n",
    "        # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.\n",
    "        centroids = revise_centroids(data, k, cluster_assignment)\n",
    "            \n",
    "        # Check for convergence: if none of the assignments changed, stop\n",
    "        if prev_cluster_assignment is not None and \\\n",
    "          (prev_cluster_assignment == cluster_assignment).all():\n",
    "            break\n",
    "        \n",
    "        # Print number of new assignments \n",
    "        if prev_cluster_assignment is not None:\n",
    "            num_changed = sum(abs(prev_cluster_assignment - cluster_assignment))\n",
    "            if verbose:\n",
    "                print(f'    {num_changed:5d} elements changed their cluster assignment.')  \n",
    "        \n",
    "        # Record heterogeneity convergence metric\n",
    "        if record_heterogeneity is not None:\n",
    "            score = compute_heterogeneity(data, k, centroids, cluster_assignment)\n",
    "            record_heterogeneity.append(score)\n",
    "        \n",
    "        prev_cluster_assignment = cluster_assignment[:]\n",
    "        \n",
    "    return centroids, cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDWls7MoqEtc"
   },
   "source": [
    "## Plotting convergence metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyZlI6lnqEtc"
   },
   "source": [
    "We can use the above function to plot the convergence metric across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WiZ0uJPqEtd"
   },
   "outputs": [],
   "source": [
    "def plot_heterogeneity(heterogeneity, k):\n",
    "    \"\"\"\n",
    "    Plots how the heterogeneity changes as the number of iterations increases.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(heterogeneity, linewidth=4)\n",
    "    plt.xlabel('# Iterations')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title(f'Heterogeneity of clustering over time, K={k}')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OIqZMfCLqEte"
   },
   "source": [
    "Let's consider running k-means with K=3 clusters for a maximum of 400 iterations, recording cluster heterogeneity at every step.  Then, let's plot the heterogeneity over iterations using the plotting function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WMu3ItP7qEtf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 3\n",
    "heterogeneity = []\n",
    "initial_centroids = get_initial_centroids(tf_idf, k, seed=0)\n",
    "centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
    "                                       record_heterogeneity=heterogeneity, verbose=True)\n",
    "plot_heterogeneity(heterogeneity, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18SGZe9FqEti"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3916,  831, 1160], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAmuWS8wqEtm"
   },
   "source": [
    "# Beware of Local Minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CC6Zs5_-qEtn"
   },
   "source": [
    "One weakness of k-means is that it tends to get stuck in a local minimum based on its starting position. To see this, let us run k-means multiple times, with different initial centroids created using different random seeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mprx1wK6qEtn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=000000, heterogeneity=5637.35477\n",
      "seed=020000, heterogeneity=5628.17552\n",
      "seed=040000, heterogeneity=5656.61492\n",
      "seed=060000, heterogeneity=5626.79996\n",
      "seed=080000, heterogeneity=5623.32338\n",
      "seed=100000, heterogeneity=5615.73065\n",
      "seed=120000, heterogeneity=5641.14122\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "k = 10\n",
    "heterogeneity = {}\n",
    "for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n",
    "    initial_centroids = get_initial_centroids(tf_idf, k, seed)\n",
    "    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
    "                                           record_heterogeneity=None, verbose=False)\n",
    "    # To save time, compute heterogeneity only once in the end\n",
    "    heterogeneity[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n",
    "    print(f'seed={seed:06d}, heterogeneity={heterogeneity[seed]:.5f}')\n",
    "    \n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gB-6CdxkqEtp"
   },
   "source": [
    "Notice the variation in heterogeneity for different initializations. This indicates that k-means runs may have not converged or they got stuck at a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEEg_0RFqEtp"
   },
   "source": [
    "One effective way to counter this tendency is to use **k-means++** to provide a smart initialization. This method tries to spread out the initial set of centroids so that they are not too close together. It is known to improve the quality of local optima and lower average runtime, but is a bit slower to start since it needs to do more computation to place centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWaf2ETPqEtq"
   },
   "outputs": [],
   "source": [
    "def smart_initialize(data, k, seed=None):\n",
    "    \"\"\"\n",
    "    Use k-means++ to initialize a good set of centroids\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    centroids = np.zeros((k, data.shape[1]))\n",
    "    \n",
    "    # Randomly choose the first centroid.\n",
    "    # Since we have no prior knowledge, choose uniformly at random\n",
    "    idx = np.random.randint(data.shape[0])\n",
    "    centroids[0] = data[idx,:].toarray()\n",
    "    \n",
    "    # Compute distances from the first centroid chosen to all the other data points\n",
    "    distances = pairwise_distances(data, centroids[0:1], metric='euclidean').flatten()\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        # Choose the next centroid randomly, so that the probability for each data point to be chosen\n",
    "        # is directly proportional to its squared distance from the nearest centroid.\n",
    "        # Roughtly speaking, a new centroid should be as far as from ohter centroids as possible.\n",
    "        idx = np.random.choice(data.shape[0], 1, p=distances/sum(distances))\n",
    "        centroids[i] = data[idx,:].toarray()\n",
    "        \n",
    "        # Now compute distances from the centroids to all data points\n",
    "        distances = np.min(pairwise_distances(data, centroids[0:i+1], metric='euclidean'),axis=1)\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJFcP_b6qEts"
   },
   "source": [
    "Let's now rerun k-means with 10 clusters using the same set of seeds, but always using k-means++ to initialize the algorithm.\n",
    "\n",
    "This may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBK4WvjGqEts"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed=000000, heterogeneity=5657.01878\n",
      "seed=020000, heterogeneity=5638.05710\n",
      "seed=040000, heterogeneity=5630.05621\n",
      "seed=060000, heterogeneity=5648.38517\n",
      "seed=080000, heterogeneity=5628.74359\n",
      "seed=100000, heterogeneity=5633.01894\n",
      "seed=120000, heterogeneity=5635.11785\n",
      "Wall time: 3.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "k = 10\n",
    "heterogeneity_smart = {}\n",
    "for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n",
    "    initial_centroids = smart_initialize(tf_idf, k, seed)\n",
    "    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
    "                                           record_heterogeneity=None, verbose=False)\n",
    "    \n",
    "    heterogeneity_smart[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n",
    "    print(f'seed={seed:06d}, heterogeneity={heterogeneity_smart[seed]:.5f}')\n",
    "\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WSvVWorqEtu"
   },
   "source": [
    "Let's compare the set of cluster heterogeneities we got from our 7 restarts of k-means using random initialization compared to the 7 restarts of k-means using k-means++ as a smart initialization.\n",
    "\n",
    "The following code produces a [box plot](http://matplotlib.org/api/pyplot_api.html) for each of these methods, indicating the spread of values produced by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgKnKieXqEtu"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.boxplot([list(heterogeneity.values()), list(heterogeneity_smart.values())], vert=False)\n",
    "plt.yticks([1, 2], ['k-means', 'k-means++'])\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.xlabel('Heterogeneity')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUF2otRSqEtx"
   },
   "source": [
    "A few things to notice from the box plot:\n",
    "* Random initialization results in a worse clustering than k-means++ on average.\n",
    "* The best result of k-means++ is better than the best result of random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ha9ga5pnqEty"
   },
   "outputs": [],
   "source": [
    "def kmeans_multiple_runs(data, k, maxiter, seeds, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs kmeans multiple times \n",
    "    \n",
    "    Parameters:  \n",
    "      - data    - is an np.array of float values of length N.\n",
    "      - k       - number of centroids\n",
    "      - maxiter - maximum number of iterations to run the algorithm\n",
    "      - seeds   - Either number of seeds to try (generated randomly) or a list of seed values\n",
    "      - verbose - set to True to display progress. Defaults to False and won't display progress.\n",
    "    \n",
    "    Returns  \n",
    "      - final_centroids          - A np.array of length k for the centroids upon \n",
    "                                   termination of the algorithm.\n",
    "      - final_cluster_assignment - A np.array of length N where the ith index represents which \n",
    "                                   centroid data[i] was assigned to. The assignments range between \n",
    "                                   the values 0, ..., k-1 upon termination of the algorithm.\n",
    "    \"\"\"    \n",
    "    min_heterogeneity_achieved = float('inf')\n",
    "    final_centroids = None\n",
    "    final_cluster_assignment = None\n",
    "    if type(seeds) == int:\n",
    "        seeds = np.random.randint(low=0, high=10000, size=seeds)\n",
    "    \n",
    "    num_runs = len(seeds)\n",
    "    \n",
    "    for seed in seeds:\n",
    "        \n",
    "        # Use k-means++ initialization with the provided seed: Fill in the blank\n",
    "        # Set record_heterogeneity=None because we will compute that once at the end.\n",
    "        initial_centroids = smart_initialize(data, k, seed)\n",
    "        \n",
    "        # Run k-means: Fill in the blank \n",
    "        centroids, cluster_assignment = kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False)\n",
    "        \n",
    "        # To save time, compute heterogeneity only once in the end\n",
    "        # Fill in the blank on the right\n",
    "        seed_heterogeneity = compute_heterogeneity(data, k, centroids, cluster_assignment)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'seed={seed:06d}, heterogeneity={seed_heterogeneity:.5f}')\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # if current measurement of heterogeneity is lower than previously seen,\n",
    "        # update the minimum record of heterogeneity.\n",
    "        if seed_heterogeneity < min_heterogeneity_achieved:\n",
    "            min_heterogeneity_achieved = seed_heterogeneity\n",
    "            final_centroids = centroids\n",
    "            final_cluster_assignment = cluster_assignment\n",
    "    \n",
    "    # Return the centroids and cluster assignments that minimize heterogeneity.\n",
    "    return final_centroids, final_cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1lgVAVLqEtz"
   },
   "source": [
    "## How to choose K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDL8IXz4qEt0"
   },
   "source": [
    "Since we are measuring the tightness of the clusters, a higher value of K reduces the possible heterogeneity metric by definition.  For example, if we have N data points and set K=N clusters, then we could have 0 cluster heterogeneity by setting the N centroids equal to the values of the N data points. (Note: Not all runs for larger K will result in lower heterogeneity than a single run with smaller K due to local optima.)  Let's explore this general trend for ourselves by performing the following analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l337zL21qEt0"
   },
   "source": [
    "Use the `kmeans_multiple_runs` function to run k-means with five different values of K.  For each K, use k-means++ and multiple runs to pick the best solution.  In what follows, we consider K=2,10,25,50,100 and 7 restarts for each setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNha4kkvqEt1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running k = 2\n",
      "seed=020000, heterogeneity=5707.49794\n",
      "seed=040000, heterogeneity=5704.51207\n",
      "seed=080000, heterogeneity=5685.53624\n",
      "Running k = 10\n",
      "seed=020000, heterogeneity=5638.05710\n",
      "seed=040000, heterogeneity=5630.05621\n",
      "seed=080000, heterogeneity=5628.74359\n",
      "Running k = 25\n",
      "seed=020000, heterogeneity=5567.14966\n",
      "seed=040000, heterogeneity=5559.25484\n",
      "seed=080000, heterogeneity=5568.99667\n",
      "Running k = 50\n",
      "seed=020000, heterogeneity=5496.77086\n",
      "seed=040000, heterogeneity=5514.12112\n",
      "seed=080000, heterogeneity=5508.93097\n",
      "Running k = 100\n",
      "seed=020000, heterogeneity=5399.81193\n",
      "seed=040000, heterogeneity=5416.88642\n",
      "seed=080000, heterogeneity=5399.67106\n",
      "Wall time: 54.6 s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAELCAYAAADqYO7XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XecVNX9//HXZ+m9w4pIExVZLCgKltiiLoktGksUjCVfNcbElG/ys8UYSxLzTTOmG6MmAUssscQEsGAhUkSKsiqKdHCXJh0Wdvfz++PcwdnZ2WWGndnZ2X0/H495DHvvuXfOHHbnM+fec87H3B0RERHJnYJcV0BERKS5UzAWERHJMQVjERGRHFMwFhERyTEFYxERkRxTMBYREckxBWMRkSwys5PMzM3sh7muizReCsbSpJjZwOiD7+kk+8zMfhXtn2VmPXJRx3SZ2RIz21DH/q7Re3olA6+zpD7nkNRl4v9Mmo6Wua6ASEMwswLgPuArwGvAWe6+Kbe1kmZiJnAwsDbXFZHGS8FYmjwzawWMBy4E/gN80d2357ZW0ly4+zbg/VzXQxo3XaaWJs3M2gFPEwLx48A5qQRiM3swuow4spb990X7j4nbdqGZTTWztWa23cyWmtnTZnZ8pt5Pusyss5ndZWbvm9kOM1sX1emwuDIDzcyBAcCA6H3FHpfHlSsws6vMbIaZbYkeb5jZeUle96Ho+P3N7AYz+8DMdsbfN41e9yEz+zjat9TM7jWznrW8l6/HvY9FZnZrdH43s4eSlN8/+n9cEZ1/hZn93sx6JZSL3dp4yMyGRO2z0cw2m9lzZrZ/LfUZYWaPm1mZmZWb2Udm9hMz65hQrto949jP0e4TE9r7JDP7QfTvL9TyundE+89Ntl/yk3rG0mSZWSfgOeBE4AHgKnevSvHw8cDlwFhgVsJ5WwPnA4vcfVq07Trgt8BHwKPAFqBv9NonAVPr927SFwW11wiXSKcAzwM9gC8Cp5nZqVH9NwC3A9+KDr0n7jRzo3MZ8AjhS827wF+j/WcAT5rZt9z910mq8VvgyOi1nwEWRec7iNAmPQhflhYARwDfAM4ws9HuvibuvfwYuAlYDvwBaAF8DRhVy3s/BpgItAWeBRYDQ4GvAsVmdpS7r084bCAwHZgH/Bk4BDgTKDKzovgvcVEgfBTYGdW/NKr/jcDJZnaCu+9MVjdgCaG9bwOWAg8l7PsQ+AHhlkq1sQ/R7ZbLgNXAv2o5v+Qjd9dDjybzIHygOvAq4V6dA78CLM3zFAArgY+BgoR950TnvSNu22xgBdA+oawB3ev5npYAO4Af1vK4O6rPKwnHPRJtvzhh+xBgI/BOktdZUksdronO9TugRdz2DsAMoBzoG7f9oaj84vjtcfunRPsvTdj+g2j7A3HbhgKVhCDVJW577+j/yIGH4ra3JgS59cDBCee/ICr/2yS/Mw78b0L5BxPbEOgJbCJ8seibUP57Ufnvxm07Kdr2w4SyNf7P4vY9B1QA+yRsL46O+1mu/9b0yOwj5xXQQ49MPhI+WB2YXI9z/SI6x6kJ2/8RbT8obtvs6MO5dRbe05KE91Tb45W4Y3pGAexftZzz59ExwxNeZ0kt5d+OgluN90foPTrw9bhtsWB8XZLy/aN9s5Psa0voZW6PvRbhC4cDVycp//+SBOPzEgNiwjGzgLVJfmc+ouYXrxOjfb+I2/adaNv5Sc5dQOi1zorbtjfB+Oxo/40J2x+Ltg9tiL8nPRruocvU0lS9DRQSLsd+05NfQt2T8YQP3rHAixDuwQJnET5sF8SVfYzQQ51vZo8ReubT3H1rPd5DvI3u3jXZDjPrCnySsPkoQmDoaMnntx4cPQ8F5tf1wmbWHhgOLANuDlesq4ndgx2a5PBZSbYdHj2/krjD3XeY2XTC1YeDgHeA2P3tN5Kca1qSbbFL18Nree/tgB5m1tPd40c4v+01b2OsjJ7j2z52/uPNbHiS8+8ieVuk43lgFXAl4fcKC1PxzgHecHcNCGtiFIylqVoMXAK8DNxjZu7u96ZzAnefY2bvAeeZ2bXuvoPQ62pLCNTx/o8QEK8Fvh89dpjZo8B33D0xWGZb9+j5xOhRmw4pnKsb4XL7AMJ9znTOtTrJts7Rc1kt5ylNKNcpel6TpGyy88fe+2W1nD+mA9WnG21MUqYiem6R5Pzf3MP595q7V5rZg8At0f3n14BxQBvg/my9ruSORlNLk+XuJcAphA/xX5vZN/biNBMIQeHM6OexhMu/jya8lrv7fe4+gtAjv4jQO76c6gN0GkpsDvWP3N3qePy1zrNUP9d/93CuK5Ic60m2xc7Xp5bX65NQbnP03CtJ2d51nP+0PdR3aS2vvyex8x9Q1/n38tzx7ie035XRz1cSBgb+IwPnlkZGwViatCggn0wIyPea2dfTPMXDhA/ES8xsH0Jwf9Hda+vV4e5l7v4P4POEQUefM7OGvgr1JqHeo9M4ppLqPUAA3H0zYZ7s8MRpO3tpbvR8QuIOM2tDuAy8gzDCGsLoZoBjk5zrmCTbZkbP6bz3dGTq/FUkae8Yd19CuD1ygZmdAhwKPJrBWx/SiCgYS5OX0EP+TTQNKdVjFxPuVX6eMC2mgJqXqDGz080s8YO1PdCRMP2lKq7sUDOr7z3FOrl7KfAE8FkzuzZJfQvMLPHy9XqgZxQQE/0G6AL83szaJjlfkZkl66Umq9sywlWDI83sooTd3wX2IQSd2NSgxwjt9z0z6xL3mr1Ifqn4acIUqBvN7OgkdW1nZkmnRKXoQUIP9admdkCS83c1sxEpnGc9sO8eytxH+D36e/TzX9KpqOQP3TOWZsHd50e9i5eB30b3kH+f4uETgOMI81y3kTD3M/IPYIuZTSVMq2lPmIO7D3BXwsCg96LnTFzKrMu1hIFEvzez/yH06LYQRjMfQ7jEGx9YpwAjgWfM7L+EgUjPu/s7hLm9xwKXAieZ2cuEe7v7EHpsh0fnTHYPt7a6TQUeNrMLgA8I83SLCff7b4gVdPf3zOznhJHT75jZE4Qe5YXAW4T53FVx5cujc/4HmG5mkwlzo1sSRk6fSBj4NSbFulbj7qvNbCzhS0KJmf2bcAWkAzA4Ov9fCV/e6jKF0Ot9jDDgsBJ4OPqyEvMMoU37AiXuPn1v6ix5INfDufXQI5MPPp2m8nQt+w8hfLhVAdemeM4ehN6tAxNqKXMtYW7oUsIl1tWEBTcuSlLWw59eyu9pCbChjv1dqWWaDCFA3AzMAbYSgvGHhDnI5yWU7UzoeZUSAoMDlyeUGUsIIp8Q5hYvAyZF779DXLmHouMH1lHvwcDfotfbGZ3rt0DvJGUNuJ5w6bqcELBvJVzSduDXSY7pz6cLsZRHdX6H0Ms/KsnvzEN1/D4l2zcsep/Lo/qvJUxxu5u4qUfUPrWpL+Hqxbro99GBk5K8zr3Rvm/n+u9Lj+w9LPrPbhBmdhLhDznR7mkb0bJ2tY2CXODuuy/vRZfL7iSMMuxKuBd1g4eRh/GvW0D4pn0NYXDNAsKCDU/W5/2ISG6Z2VcIA52u89SvdOQVM3uN8KVjX68+FUuakFxdpr6eMMAkpiLu33cCf0woP5DwTf7ZhO1/IVwK/B5hwYXrgElmdoy7z40rdyfhXtQthMtaXwIeN7Mz3f3f9XsrIpJt0f3h9e5eGbdtH8LfdBVNdGlIMzsS+AzwiAJx05arnvFp7v5iGsfdCtxBWC2oJNp2GKEnfKW7PxhtawmUEHrQZ0fbehMuI93t7rfFnfMloJe7H5qJ9yYi2WNm3yV8iY/dq96P8EW8C+Ge/K05rF7GmdklhPv9lxPuyx/h4d69NFH5MoDry8BbsUAcOZswwOSx2AZ3r4gWWbjRzNq4ezlhQEhrao6AHQ88YGaDPIyYFZHGayphYFQxYdGNnYT7v39097/lsmJZcjWhR7wI+LICcdOXq2A8Icoos4Ew+ONGrz6CcDczO46wsP31CbuKgMUecoXGKyEE3yHRv4sIgzcWJikHYRCGgrFII+ZhFPFZua5HQ3H3k3JdB2lYDR2MNxIW33+VsIrNCMJIz2lmNsLdk02L+DKhB/xIwvbu1FyPF8Lcvdj+2PMGr3k9PrFcDWZ2NeEbKh06dDhy6NCsTg0VEZEm5q233lrr7slWj6umQYOxu88hTLGIeTUaKTiT0PP9fnz5aPGBCwmZZxIHLxjJl9pLnLuZarlk9b2PMOmekSNH+qxZyda8FxERSc7MUlp2NecrcLn7bMKE/6OS7D6HMGUp2fq560neq+0Wtz/23M1qpppJLCciIpITOQ/Gkdp6r5cRJtInm35UAgyK0rvFG0YY3LEwrlwbYP8k5SCszCMiIpIzOQ/GZjYSOBCYkbC9D3A6YXm4XUkOfRZoBVwQd0xLQracydFIaoCJhOA8NuH4ccB8jaQWEZFca9B7xmY2gTByeTZhJPUIwnq/KwlL1MUbG9UvaYo3d58brel6j5m1is57LTCIuMDrYR3ZXwE3mdnm6LUvIiQOOCdz705ERGTvNPRo6vnAxcA3CAvplwJPAbclGaB1GaHnOruO810B/Ai4i3BveR4wJskxtxDW5P0mny6HeaG7P1e/t5OeKQtWU1npnDqstjSuIiLSHDXoClz5rL6jqf8+bQm3PVtCm5Yt+Mc1x3BIvy57PEZERPKbmb3l7iP3VC7n94ybgx//+z1ufaaEKoftuyr5yl/fZNWG7bmuloiINBIKxg2gf/fqA75Xby7nyofeZEt5RS1HiIhIc6Jg3ADGjR7AVZ8ZVG3b+6Wb+frDs6morKrlKBERaS4UjBvIjZ87mNMTBm69smANtz/3LrpvLyLSvCkYN5AWBcY9XzqcQxMGbv19+lIe+O+S3FRKREQaBQXjBtS+dUvu//JI+nZpW237Xc+/y+SS0hzVSkREck3BuIH17tyWB644io5tPp3i7Q7ffHQu76zYmMOaiYhIrigY58DQws78buwRtCj4NHeFpjyJiDRfCsY5cuKBvbjjnKJq2zTlSUSkeVIwzqGxozTlSUREFIxz7qbPHUxxkaY8iYg0ZwrGOVZQYNxz0QhNeRIRacYUjBuBdq1bcP9lI9m3a7tq2zXlSUSkeVAwbiR6d2rLA5drypOISHOkYNyIHFTYSVOeRESaIQXjRqauKU+bd+zKUa1ERCSbFIwbobGjBnD1CYOrbQtTnuZoypOISBOkYNxI3ThmaI0pT69+sIYfPleiKU8iIk2MgnEjFZvydFjClKfx05fxl6mLc1QrERHJBgXjRqxd6xb8OcmUpx/9+z1NeRIRaUIUjBs5TXkSEWn6FIzzgKY8iYg0bQrGeUJTnkREmi4F4zyiKU8iIk2TgnGe0ZQnEZGmR8E4z2jKk4hI06NgnIc05UlEpGlRMM5TsSlPnTTlSUQk7ykY57Hapjxd+dc3WakpTyIieUPBOM+dcGAv7jxneLVtazaX8xVNeRIRyRsKxk3AJaP6a8qTiEgeUzBuIm4cM5QxRYXVtmnKk4hIflAwbiIKCoxfXXS4pjyJiOQhBeMmRFOeRETyU1rB2MyuMrMO2aqM1F9tU56uf3QOT89ZmcOaiYhIbdLtGf8RWGVmvzOzQ7NRIam/ZFOeduyq4luPzeXmf77Djl2VOaydiIgkSjcY7w/8HjgPmGNm08zsMjNrm/mqSX0km/IE8PCMZZz/xzdYtm5bDmolIiLJpBWM3X2Ju98E7Ad8CdgGPEDoLf/KzA7OQh1lL10yqj8//eIhtG5Z/b95/spNnPGb15mk+8giIo3CXg3gcvcKd3/c3T8LHAS8DVwPzDezV83sjExWUvbeRUf1559fO5YBPdpX2755RwXX/P0tfvT8u+zSXGQRkZza69HUZtbJzL4GPAmcAMwBbgFaAs+a2R2ZqaLUV1HfLjz3jeNrzEMG+PPri7n4vul8vFHLZ4qI5ErawdjMRprZn4FVwM+BucAx7j7S3e929+OAHwLXZbSmUi+d27biD+OO4NYzh9EybmAXwKyln3DGvVN57YM1OaqdiEjzlu7UpreAGcDJwB1AP3e/zN1nJBR9AeiWmSpKppgZXzl+EI9dcwz7dKk+5m791p1c9uBMfvnCB1RWacUuEZGGlG7PeBVwJnCAu//M3dfXUm42MKheNZOsOXJAN56//jOceGCvatvd4d6XPuSyB2aydkt5jmonItL8pBuMfwa87kkWOzazjmZ2AoC773T3pZmooGRH9w6tefDyo/ju6QeScNWaqQvX8vlfv87MxbV91xIRkUxKNxhPAYbVsu+gaL/kiYIC4+unHMD4r4yiZ8c21fat3lzOxX+ezh9f/UiJJkREsizdYGx17GsDaGmnPHTskJ78+/rjOXpQ92rbK6ucu//zPlf9bRYbtyk3sohItuwxGJvZQDM7xcxOiTaNjP0c9zgD+F9gWVZrK1nTu3NbHv6fUVx70v419r343mrO+M3rvL1iQw5qJiLS9NmeLkGa2W3AbUB8wfgeskc/VwDXufufM13JxmDkyJE+a9asXFejQbz8fhnffmweG7dX7w23blHArWcezLjRAzCr6yKJiIhAmIXk7iP3VC6Vy9QPEaYyfZYQdL8e/Rx7nAIcCxTuKRCb2Ulm5kkeNbpcZjbazCaa2QYz22pm75jZlxLKtDWzn5nZx2a2PVor+4Qk5yows5vMbImZ7TCzeWb2xRTee7N0ytA+PH/98Ry2X9dq23dWVnHrMyVc/+hctpRX5Kh2IiJNT8s9FYhGRS8FMLOTgdnuvrmer3s98Gbcz9U+2aPL3v8EHgYuAXYSBo4lJqT4C3AG8D1gEWGhkUlmdoy7z40rdyfwXcIKYW8R1tV+3MzOdPd/1/O9NEn9urXn8WuO4cf/fo+H3lhSbd9z81ZRsmojfxh7JAcVdspNBUVEmpA9XqbO6IuZnUQYcX2au79YS5lOwEfAw+7+rTrOdRhh9a8r3f3BaFtLoARY4O5nR9t6A8uBu939trjjXwJ6uXtKqSCb02XqRP96exU3PPE2W3dWH5/XtlUBd33hEM4/sl+OaiYi0rhl7DK1mS2KAh9mtjj6ubbHRxmo+wVAL+AXeyh3NrALeCy2wd0rgEeBYjOLzdUpBloD4xOOHw8cYmZanGQPzjy0L89943iGJvSCd+yq4ruPz+OGJ95WjmQRkXpI5Z7xq8CmuH/X9XgtxdedYGaVZrbOzB42s/5x+44H1hMC5TtmVmFmy83sNjNrEVeuCFjs7omJeUsIwXdIXLlyYGGSclD7vGmJM7hXR/75teO4IEkv+LFZyzn392+weO3WHNRMRCT/pXLP+Iq4f19ez9fbSOjxxgL8COBmYJqZjXD31UBfoD3hfvGdhHu8pwK3Al2Bb0fn6g58kuQ11sftjz1vSLJqWGK5GszsauBqgP79+9dWrNlo17oFP7vgMI4a1J1bn55PecWnqRff+3gTZ/1mKj87/1A+d8g+OayliEj+2esUinvD3ee4+3fd/Tl3f9Xd7wHGAH0Ig7pidWoL3OHuv3D3V9z9+8CfgevMrEtUzqg+3Yq47Yk/p1IuWX3vi7JRjezVq9eeijcbF47cj6evO47BPTtU276lvIJrJ8zm9udK2FmhHMkiIqnamxSKI8zsKTNbG11CPiLa/mMzG5Pu+dx9NvABcFS0aV30/EJC0clAK8JlZwg922S92m5x+2PP3azmxNjEcpKGg/fpzDNfP44zDq3ZC37wv0u48E/TWLlBOZJFRFKRbgrF44FpwFDCZeT446uAr+5lPeJ7r7F7uYm92VgwrYorN8jM2ieUG0aYCrUwrlwbIHFpqdi94nf3ss7NXqe2rfjtxSO4/ewiWrWo/l1n7vINnHHv60xZsDpHtRMRyR/p9ozvBiYReqffSdg3Gzgi3QqY2UjgQEKeZICno+fEXnYxsAOYH/38LKGnfEHcuVoCFwGT3T2WA3AiITiPTTjfOGC+uy9Ot87yKTPjsmMH8vhXj2Xfru2q7duwbRdXPPgmP5v0PhWVumwtIlKbPQ7gSnAEcJ67u5kl9lzXEqYk1crMJgCLCYF7A2EA103ASuA3AO4+38weAu4ws4Ko7KnA/wB3uvuWqNxcM3sMuMfMWkXnvZaQR3l34HX31Wb2K+AmM9scne8iwsph56T5/qUWh+/XleevP57v/GMeL79fvTf8uykf8dbST7j34hH07pS4bouIiKQbjHcQRjonsw9htHRd5gMXA9+IzlMKPAXc5u5r48pdQwjQ3yAM7loCfMfdf51wviuAHwF3EUZazwPGRPeh490CbAG+CRQCC4AL3f25PdRX0tC1fWvu//JI/vjaR/x80gKq4r6uTV+0ns//eiq/uXgEx+zfI3eVFBFphNJagcvMniUEvZOjTbuAI919jplNBta6+yWZr2buNecVuPbG9EXruP6ROazeXF5te4HB/55+ENeeuD8FBUo2ISJNWyYTRcS7lXCpel70bwcuM7MpwGjg9nQrKk3T6ME9eP76z3BsQi+4yuFnkxbwlb++ySdbd+aodiIijUtawdjd5wEnAGWES7+xLE4AJ7r7gsxWT/JZr05t+PtXRvGNU4bU2DdlwRrO/M1U5ixLtm6LiEjzkvY8Y3ef7e6fBToB/YDO7n6yu8/JeO0k77UoMP739IN46Iqj6Na+VbV9Kzds58I/TeOBqYtpyIQlIiKNzV6vwOXuO9x9VZK1oUVqOOmg3jx//Wc4on/1HMm7Kp07/vUu1z08m807duWodiIiuZV2CkUzGwxcCPSnZn5hd/evZKhujYoGcGXGzooqfjrxff4yteb07oE92vP7sUcyrG/nHNRMRCTzUh3Ale5o6nOAxwk96tWEbEjx3N0Hp1PRfKFgnFkT53/M9x5/m83lFdW2t2lZwB3nFHHhyP2ouYKpiEh+ydZo6ruAV4B93L2vuw9KeDTJQCyZN2b4Pjz3jeMZtk/1XnB5RRU3PPkO3338bbbvVI5kEWke0g3Gg4Gfu/uabFRGmpeBPTvw1NeO5eKj96ux78nZK/jC7/7LwtVbclAzEZGGlW4wfh/Q8kmSMW1bteAn5x3KLy88jHatWlTbt6BsM+f8dirPzluVo9qJiDSMdIPx/wNujgZxiWTMeUf045mvH8f+varnSN66s5LrH5nDrU/Pp7xCl61FpGlKdwDX64RUhD2AD6mZC9jd/cTMVa/x0ACuhrG1vIKb//kOz8yt2Rs+tF8XfnfJEezXvbbl0UVEGpdsDeCqJCRZeANYE/0c/1CePKmXDm1acs9Fh3PXF4bTukX1X8+3V2zkjHtf58V3y3JUOxGR7Eh7nnFzpZ5xw3tnxUa+9vBbLF+/vca+a04czPdOP4iWLfZ63RoRkazLVs9YpMEc0q8L//r6ZzhtWJ8a+/706iIu+fMMSjfuyEHNREQyK+1gbGb7mtkvzWyWmS02s+HR9m+Z2ajMV1Gasy7tW3HfpUdyy+cPpkVCysWZS9Zzxr2vM/XDtbUcLSKSH9IKxmZWBLwDXAqsIiyJ2TraPQD4ZkZrJwKYGVedMJjHrh5NYefqK7Cu27qTSx+YwbcencOsJeuVcEJE8lK6PeNfAO8Bg4DzCCkUY94g5DQWyYqRA7vz/PXH85kDelbb7g5Pz13F+X+cxud+/Trjpy9lS8IymyIijVm6wfh44G533wIkdkHKgMKM1EqkFj06tuGhK47mW6ceQLKlq98v3cz3n57PqB+9yPeffof3Szc1fCVFRNKUbjCua+pST6DmsFeRDGtRYHzr1AP5+5WjGNgj+ZzjrTsrGT99GWPueZ3z//AGT89ZqUVDRKTRSnfRjxeBTe5+npm1AHYBI919tpk9CrR397OzVNec0tSmxqmqynl94VrGT1/KS++VUVXHr3P3Dq25YGQ/xh49gP61BHERkUzKVgrFE4EXgSnAw8BfgJuAIuBLwAnuPmOvatzIKRg3fqs2bOeRmct49M3lrNmcmN3zU2Zw4oG9GDdqACcP7V1jlLaISKZkJRhHJz4DuIewLGbMEuA6d/9PWifLIwrG+WNXZRWTS8oYP30p0xatq7Psvl3bcfHR+3HhUfvRu1PbOsuKiKQra8E47gWGAL2Bde6+YK9OkkcUjPPTwtVbmDBjKU+8tYLNO2ofYd2ywCgeXsilowcwalB3LNnoMBGRNGU9GDc3Csb5bfvOSp6bt4rxM5by9oqNdZY9oHdHxo7qz3lH9qNz21YNVEMRaYqydc/4y3XsrgI2AnPcfUXKJ80TCsZNx7zlGxg/fSnPzltFeUXtEwTatWrBOYf3ZdzoAQzft0sD1lBEmopsBeMqPp1fHH8dL35bFfAYcIW770z55I2cgnHTs3HbLp6YvYIJM5ayaM3WOssetl9Xxo3qz1mH9aVtqxYNVEMRyXfZCsbHABOA54AnCAt99AEuBM4EvgYMB24H7nH3m9OveuOkYNx0uTvTPlrH+BlLmVxSRkUd86O6tGvFBUf2Y+zoAQzq2aEBayki+ShbwfhJYEGyIGtmPwYOdvdzzexOYKy7D06n0o2ZgnHzULZpB4/OXM4jM5dRuqnujFDHD+nJuNH9OfXgPkrlKCJJZSsYbwLOdfeXkuw7FXjK3Tub2WnAv9y9TTqVbswUjJuXisoqXnp/NeOnL+X1PWSF6tO5DV86qj8XH92fwi6aHiUin0o1GLdM87w7gSOBGsE42h67R1wA1H0TTqQRa9migOKiQoqLClmydisPz1zGP2YtZ8O2XTXKlm0q59cvfchvpyzktIP7MG70AI7dvwcFWkxERFKUbjB+HLjdzCoJ94xXE+YaXwD8EHggKnc40OTnHkvzMLBnB27+/MF857QDef7tjxk/Yylzlm2oUa6yyplYUsrEklIG9ezA2FH9Of/IfnRt3zrJWUVEPpXuZep2wJ+Bi5Psfhi4yt13RKt0bXb31zJTzdzTZWqJV7JqI+OnL+OZuSvZtrP2BBRtWhZw5qF9GTe6P4fv11WLiYg0M1ld9MPMDiTkLi4EPgZmuPsHaZ8ojygYSzKbduzi6TkrGT99KR+UbamzbFHfzlw6egBnH96X9q3TvSglIvlIK3BlmIKx1MXdeXPJJ4yfvpT/zP+YXZW1/111atuSLx7Rj7Gj+nNAn04NWEsRaWjZTBTRHrgSOBHoDqwDXgEecvdt6VcVNfDMAAAdw0lEQVQ1PygYS6rWbC7nH7OW8/CMZazcUHeK71GDujNu9ACKiwpp3VLTo0SammxNbSokBN4DgaVAKeFS9QDCgK2T3L1sbyrc2CkYS7oqq5xXP1jN+OnLmLJgNXX9qfXs2IYvHbUfF4/qz75d2zVcJUUkq7IVjP8GFAPnuft/47YfCzwJTHL3y9OvbuOnYCz1sXz9Nh6ZuYzH3lzOuq21rxJbYHDK0N6MHT2AEw/opelRInkuW8F4DXCDuz+QZN9XgLvdvVdaNc0TCsaSCeUVlUycX8qE6cuYuWR9nWX3696OS44ewIUj+9GjY5NZP0ekWclWMN5OWIFrYpJ9xcDT7t4kr7EpGEumLSjdzIQZS3lq9kq2lNeea7l1iwI+d0gh40YPYOSAbpoeJZJHshWM5wIl7j42yb6/A8PdfURaNc0TCsaSLVvLK3hm7irGT1/Kux9vqrPs0MJOjB09gHNH7EvHNpoeJdLYZSsYjwP+BrxMWOTjY8IAri8BpwKXuvvDe1XjRk7BWLLN3ZkT5Vr+19sfs7OOXMsdWrfgCyP2ZdzoARy8T+cGrKWIpCObU5uuBu4gLIMZUwb8wN3/nNbJ8oiCsTSkT7bu5PG3ljNhxjKWrqt7xuCRA7oxbnR/Pjd8H+VaFmlksr0CVwFwEGGe8XpCWsXav8Y3AQrGkgtVVc7UhWsZP30pL75XRh2pluneoTUXjOzH2KMH0L9H+4arpIjUKuPB2MxaA9OBG919cj3rl3cUjCXXPt64nUdmLufRmctYvbm81nJmcMIBvRg3egCnDO1NC02PEsmZbN0z/gT4oru/XJ/K5SMFY2ksdlVW8cK7ZYyfvpQ3PlpXZ9m+Xdpy8dH9uejo/ejdSbmWRRpatoLxP4BF7n5jfSqXjxSMpTH6aM0WJkxfxhNvLWfTjtqnR7UsMIqHFzJu1ABGD+6u6VEiDSRbwfgzwHhCXuOnCaOpq53A3RelV9X8oGAsjdn2nZU89/YqJkxfyrwVG+ssO6R3R8aO6s95R/SjS7tWDVRDkeYpW8E4fpBW0gPdvUkO51Qwlnzx9oowPerZeavYsav2cZXtWrXg7MP6Mm70AA7p16UBayjSfGQrGF+2pzLu/tc6jj8JmJJk10Z37xqVGQgsruUU3dx9Q9z52gJ3AuOArsBcwnKdryW8bgFwA3ANYV70AuAOd39yT+8nRsFY8s3Gbbt4cvYKxs9YyqI1W+sse1i/LowdPYCzDu1Lu9ZN8vu0SE40ynzGccH4euDNuF0V7j4rKjOQEIx/AjybcIo33b0y7nwTgDOA7wGLgOuAzwHHuPvcuHI/Ar4L3AK8RVik5CrgTHf/dyp1VzCWfOXuTFu0jgnTlzGppJSKOuZHdWnXivOPDLmWB/fq2IC1FGmaGmKe8TCgBzDL3ev+2v3pcScRgvFp7v5iLWUGEoLxVe5+fx3nOozQE77S3R+MtrUESgjzns+OtvUGlhOSWNwWd/xLQC93PzSVuisYS1OwetMOHntzOQ/PXMbHG3fUWfa4IT0YN2oApw7rQ6sWyrUssjdSDcZp/4WZ2XWEPMZvE5bFPCja/rSZXZ/u+erhbGAX8Fhsg7tXAI8CxWYWS3NTDLQmDDyLNx44xMwGNUBdRRqF3p3b8o3PHsDr/+9k7rv0SE44sPYka/9duI5rJ8zm+J++zC9f+ICPN25vwJqKNC9pBWMzuwr4NWEk9YVA/PyI14EvpniqCWZWaWbrzOxhM+ufpMxPzKzCzDaa2bNmdkjC/iJgsbsnrhVYQgi+Q+LKlQMLk5SD0MMXaVZatijg9KJC/nbl0bz6vZO45oTBdGuffGR12aZy7n3pQ47/6RSu+fssXv9wDVV1LQUmImlLN+3Ld4BfuPsNZpY4yuN9wr3bumwEfgG8CmwCRgA3A9PMbIS7ryYEzj8Bk4E1wNCozBtmdrS7vxedqzvwSZLXWB+3P/a8wWtej08sV0O0DvfVAP37J/u+IJL/BvTowE2fP5hvn3Yg/5n/MX+ftpTZyzbUKFdZ5UwqKWNSSRn9urVjTFEhY4YXckT/bhRolS+Rekk3GA8CJtWybythRHOt3H0OMCdu06tm9howkzCo6/vu/jHw1bgyr5vZREJP9hbCyGkIvfJkX88TPxVSLZesvvcB90G4Z7yn8iL5rG2rFpw7oh/njujHu6s2MX7GUp6es5JtOytrlF3xyXbun7qY+6cuplenNpw+rA/FRYWMHtyD1i11f1kkXekG47XAwFr2HQSsTLcC7j7bzD4AjqqjzHIzm5pQZj2QrLvaLW5/7LmbmVlC7zixnIhEhvXtzI/PPYSbPjeUp+esZPz0ZSwo25y07JrN5UyYsYwJM5bRuW1LTj24D6cXFXLigb00TUokRekG4+eAH5jZK8DSaJubWU/g24R7yXujtt5rXWVKgHPNrH3CfeNhwE4+vUdcArQB9qf6fePYveJ397LOIk1ep7atuPSYgYwbPYBZSz9h/PSl/Gd+aa25ljftqOCpOSt5as5K2rYq4KQDe1M8vA+nDO2j1b5E6pDuoh89gDeA/YAZwAnRz0OB1cCx7l73Wnw1zzkyOtdd8VOPEsr0B+YD/3T3y6JthxMueV8eW2gkmtr0DrDQ3c+KtsWmNv3Y3W+PO+eLQB93TxwYlpSmNokEW8sreGXBGiaWlDLl/dVsKa99TeyYlgXGsUN6UlzUh9OG9VHSCmk2sjbP2Mw6Ad8iTBnqDawDJgK/cvdNezh2AmEO8WxgA2EA103ANuAId19rZr8gjPKeRhjAdVBUpgswyt0XxJ3v0age34vOey1wJuFLwey4cndHdb45eu2LCKtxnePuz6XyvhWMRWoqr6jkjYXrmDi/lBfeK2P91p17PMYMRg7oRnFRIcVFhezXXbmXpelqrCtw3QRcDAwA2hPmK/8HuC0auIWZXUkIqkOAToT71C8Dt8cH4qhsO+BHwCWEwWPzCMthvpJQrgUhoF9F9eUwn0i17grGInWrqKxi1tJPmDi/lMklpazaw6IiMUV9OzOmqJDi4YUc0LujMkpJk5KttakXAee6+7wk+4YDz7r74LRqmicUjEVS5+68s3IjE+eXMrGkdI9rY8cM7tmB4uGhx3xYvy4KzJL3spm1abS7z0yybyQwQ1mbRCTRwtWbmTi/lEklZbyzMrVhJft0aUtxUSGnF/Xh6IHdaaklOSUPZTMYj3L3N5Ps+yrwI3fvkVZN84SCsUhmrPhkW7R4SClvLllPKh9B3dq34rRoLvNxQ3rStlWT/M4vTVDGgrGZfZswbQlgX8KgqsRRGu0IK1k96u5j069u46dgLJJ5azaX8+J7ZUycX8obH61lV+WeI3OH1i04eWhviosKOXlobzq2SXeGpkjDyWQwPgf4QvTjZcC/CQE5Xjlhvu79SdaKbhIUjEWya9OOXUx5fzUT55fyyoI1bN9Vc+WvRK1bFvCZIT0pLirk1GF96N6hdQPUVCR12bpM/SBhFPLi+lQuHykYizScHbsqee2DMJf5pfdWs3H7rj0eU2AwalAPiov6UDy8kH26tGuAmorULetTm8ysIyGf8Sp33/NfSp5TMBbJjV2VVcxYtJ6JJR8zuaSM1ZvLUzrusP26hilTRX0Y3Ktjlmspklw2F/04E7gDOCzadFS0vvT9wMvu/nDatc0DCsYiuVdV5cxZvoFJJaVMnF/KsvWp3RU7sE9HxhQVcnpRIUV9O2vKlDSYbF2m/gLwJPASIcXh/wEjo2B8C3CCuxfvZZ0bNQVjkcbF3Xm/NDZlqpT3S5MnskgUS/9YHKV/bKH0j5JF2QrGc4C33P1/onWgd/JpMD4H+L2777vXtW7EFIxFGrcla7eGHnNJKXOS5GNOpmfHNpxeFKZMHaP0j5IF2QrGO4Cz3P2FaInJXXwajE8AJrt7k1wBXsFYJH+UbtzBC++GwDx90Xoqq/b8OdcpSv9YXNSHEw7sRfvWmjIl9ZdqME73t20T0LOWfQOpOeVJRKTBFXZpy6XHDOTSYwbyydadvBRNmXrtwzW1pn/cvKOCf85ZyT+j9I8nHtiL4qJCPju0D13aK/2jZFe6PeMJwCGE1ImbCT3jIwlzjF8H5rr71VmoZ86pZyyS/7aWV/DqB2uYOD+kf9ycYvrHY/bvwZjhhUr/KGnL1mXqgcBMwAmLf3wZeAI4lJDicKS7r9qL+jZ6CsYiTUt5RSVvfLSOSfNLeeHdMtalmP7xyP7dGDNc6R8lNdmc2tQPuJ2a+Yx/4O7L96KueUHBWKTpqqxyZi1Zz8SSUibNTz3947B9Ou8OzAf2UfpHqalR5jPOZwrGIs2DuzN/5SYmlnzMxPmlfJRi+sdBPTtQHC0ycli/rhRoypSQ2bWpf5DG67q735lG+byhYCzSPC1cvZlJJSGZRarpHws7t929LKfSPzZvmQzGyYYeOpDsa58rn7GINFUrPtnG5JIyJpaUMmvJelKYMUW39q2iKVOFHH+A0j82N5kMxom/OS2B7cAoYHZieXffc6qVPKRgLCLx1m4p58V3Q2D+78LU0z+eNLQ3Y5T+sdnI5gCuaot97GX98o6CsYjUJpb+cVJJKVPeTzH9Y4sCjj+gJ2OU/rFJUzDOMAVjEUnFjl2VvP7hWibOL+XF98pSTv949KDuu5NZ9O2q9I9NhYJxhikYi0i6dlVWMXPx+t3JLFJO/9ivC8XRlKn9lf4xrykYZ5iCsYjUR1WVM3fFBibND2tmL12XWvrHA3p33D2XWekf808mB3ANTtjUAlgAnAOUJJZ390Vp1DNvKBiLSKbE0j/G8jKnmv5x367tGDO8kDFK/5g3Mj21KbGQJdkGgKY2iYikZ+m6rbsD8+yU0z+25rRhITAr/WPjlclgfFk6L+zuf02nfL5QMBaRhlC2aQeT3y1j0vxSpi1al3L6x88O7c2Y4YVK/9jIaDnMDFMwFpGGtmHbTl56bzUTS0p57YM1lNeS/jFe21YFnHBAL8YMV/rHxkDBOMMUjEUkl2LpHyeVlPLye+mlfywuKuT0YX3o3VnpHxuagnGGKRiLSGNRXlHJtI/WMamklMklqad/PKJ/N8YUhZHZ/Xso/WNDUDDOMAVjEWmMYukfJ5WUMamklJUbtqd03MH7dGZMURgApvSP2aNgnGEKxiLS2MXSP04qCXOZF67ektJxA3u0373IyOFK/5hRCsYZpmAsIvlm4eotTCoJq3+9vSK19I99OrehuKiQMUWFHD1I6R/rS8E4wxSMRSSfrdywncnRXOY3U0z/2DVK/zhG6R/3moJxhikYi0hTEUv/OKmklKkppn9s37oFJx/Um+LhhZx8UC86tdWUqVQoGGeYgrGINEWx9I+TS8qYsmA123amlv7xuCE9GDO8kFMP7kOPjm0aoKb5ScE4wxSMRaSpi6V/nFQS0j9u2JZa+sejBnbfncxC6R+rUzDOMAVjEWlOYukfYwPAyjallv7x0H5dwgCw4Ur/CArGGadgLCLN1e70jyWlTJpfypIU0z8O6d1x91zm5pr+UcE4wxSMRUTCXOYFZZuZNL+MiSWlvPfxppSO27dru9095iMHNJ/0jwrGGaZgLCJSUyz946SSMt5a+klKx4T0j30oLirk2P17Nun0jwrGGaZgLCJSt1j6x8klpUz7aB0VqaR/bNOSUw7uzZiiQk48qOmlf1QwzjAFYxGR1MXSP04qKeXVFNM/tmlZwAkH9mJMUSGfPbg3Xdu3boCaZpeCcYYpGIuI7J1tOyt4dUFI//hSGukfRw/uEdbMzuP0jwrGGaZgLCJSfzsrqnjjo7VMKinjhXdLWbsltfSPI/brunsu84AeHRqgppmhYJxhCsYiIplVWeW8tfQTJs4vTSv949DCTowZHkZmH9SnU6OeMqVgnGEKxiIi2ePulKzatDswf5hi+scBPdozpqiQ4uGNM/2jgnGGKRiLiDScWPrHySWlzEsj/ePpw0KP+ehB3WnVCNI/KhhnmIKxiEhuxNI/TiopZebi1NI/dmkXpX8cXshncpj+UcE4wxSMRURyb92Wcl58r4xJJWVM/XAtOyv3PGWqfesWnHRQL4qLCjllaO8GTf/YKIOxmZ0ETEmya6O7d63lmD8BVwMT3H1cwr62wJ3AOKArMBe4wd1fSyhXANwAXAMUAguAO9z9yVTrrmAsItK4bN6xiynRlKkp76ee/vHYIT0YU1TIqcP60DPL6R9TDca5WurkeuDNuJ+TTjozs2OBsUBti5/+BTgD+B6wCLgOmGRmx7j73LhydwLfBW4B3gK+BDxuZme6+7/r80ZERCQ3OrVtxdmH9eXsw/qyY1clUz9cy8Q9pH/cWVnFKwvW8MqCNRT88x1GDuy+ewDYvjlM/5irnvFp7v7iHsq2AuYAEwg92qnxPWMzO4zQE77S3R+MtrUESoAF7n52tK03sBy4291vizv+JaCXux+aSt3VMxYRyQ8VUfrHiWmmfzxk3y675zIP6Z2Z9I+NvWeciu8BLYBfEIJxorOBXcBjsQ3uXmFmjwI3mlkbdy8HioHWwPiE48cDD5jZIHdfnI03ICIiDa9liwKOHdKTY4f05IdnFTFvxYYQmPeQ/vGdlRt5Z+VGfjZpAfv36sCY4YVcNLI//Xu0z36ds/4KyU0ws57ABmAScKO7L4vtNLP9ge8DZ7j7zlomdBcBi909sWVLCMF3SPTvIqAcWJikHMAwQMFYRKQJKigwRvTvxoj+3bhxzFA+KNuyey7zu3Wkf/xozVZ+N+UjRg3q0SSD8UZCT/dVwn3gEcDNwDQzG+Huq6NyfwSecvdkg71iugPJ8nWtj9sfe97gNa/HJ5arwcyuJgweo3///nVURUREGjsz46DCThxU2IlvnnoAy9ZtY1JJKRNLSpm97BMSo0Tnti0ZPbhHg9StQYOxu88h3AeOedXMXgNmEgZ1fd/MxgFHAUP3cDoDkt3wTuxGp1ouWX3vA+6DcM94T+VFRCR/9O/RnqtOGMxVJwxmdZT+cVJc+sfPHtynwXIt5/yesbvPNrMPgKPMrCPwS+CnwA4zi013KgBaRT9vdfddhJ5tsu5qt+h5fdxzNzOzhN5xYjkREWmmenduy7jRAxg3egAbt+3ipffLGjQhRe7XCgtivdeeQC/gx4RL0LHHfsCF0b/PiI4pAQaZWeLF/GHATj69R1wCtAH2T1IO4N2MvQsREcl7Xdq34rwj+nHkgG57LpwhOQ/GZjYSOBCYAZQCJyd5lAEvRv+eGh36LNAKuCDuXC2Bi4DJ0UhqgImE4Dw24aXHAfM1klpERHKtQS9Tm9kEwsjl2YSR1COAm4CVwG/cfQfwSpLjdgBl7r57n7vPNbPHgHuiOcmLgWuBQcQFXndfbWa/Am4ys83Ra18EnAKck4W3KSIikpaGvmc8H7gY+AbQntATfgq4zd3X7sX5rgB+BNxFWA5zHjDG3WcnlLsF2AJ8k0+Xw7zQ3Z/bmzchIiKSSUoUkSKtwCUiIulKdQWunN8zFhERae7UM06Rma0BlqZQtCewN5fcmyO1VWrUTqlRO6VG7ZSaTLXTAHfvtadCCsYZZmazUrkkIWqrVKmdUqN2So3aKTUN3U66TC0iIpJjCsYiIiI5pmCcefflugJ5RG2VGrVTatROqVE7paZB20n3jEVERHJMPWMREZEcUzAWERHJMQXjDDCz/czsCTPbaGabzOwpM0uW3rHZMLPzzexJM1tqZtvNbIGZ/cTMOiWU62Zm95vZWjPbamYvmtkhuap3rpnZRDNzM7srYbvaCTCzz5vZa2a2Jfpbm2Vmp8Ttb/btZGbHmdlkM1sdtdFsM7syoUxbM/uZmX0c/X1OM7MTclXnbDKzfmb2m+g9bov+vgYmKZdSm5hZgZndZGZLzGyHmc0zsy/Wt54KxvUUpXB8GRgKXAZcChwATDGzhkuG2fh8F6gEbgbGAH8gJPJ4wcwKAMzMCNm3xhDWK/8iIRPXFDPrl4tK55KZXQwclmS72gkws2uAZ4C3gHMJGdseJ6xzr3YCzOxQQoa7VsBVhDZ4E/iLmV0bV/Qv0f4fAGcCHwOTzOzwhq1xgxjCpyl4X6+jXKptcifwQ+C3wOeA6cDjZvb5etXS3fWox4OQfKISGBK3bRBQAXwn1/XLYbv0SrLty4S81adEP58T/XxyXJkuwHrg3ly/hwZur66ExCkXR21yV9y+Zt9OwEBgO/CtOsqonUIu+J1Ax4Tt04Fp0b8Pi9rpirj9LQkJdJ7N9XvIQpsUxP37f6L3PjChTEptAvQGyoHbE45/CXi7PvVUz7j+zgamu/vC2AYPOZL/SzNO0ejua5JsfjN63jd6PhtY5e5T4o7bCDxH82u7/wNK3P2RJPvUTnAlUAX8sY4yaidoDewifHGJt4FPr4SeHZV5LLbT3SuAR4FiM2vTAPVsMO5elUKxVNukmNDG4xOOHw8cYmaD9raeCsb1V0RIDZmoBBjWwHVp7E6Mnt+Lnutqu/5m1rFBapVjZnY84arB12oponaC44H3gS+Z2UdmVmFmC83surgyaid4KHq+18z6mllXM7sK+Czwq2hfEbDY3bclHFtCCDRDGqSmjUuqbVJE6BkvTFIO6vGZr2Bcf90J9yISrQe6NXBdGi0z2xe4A3jR3WO5KOtqO2gG7WdmrYA/AT939wW1FGv27QT0JYzF+BlwN3A68ALwWzP7ZlSm2beTu88HTiJcCVhJaI/fAV9190ejYntqp+5ZrmZjlGqbdAc2eHRtuo5yaWu5twdKNclWTrEGr0UjFfVIniHcR78ifhdquxuAdsCP6iijdgodh07A5e7+VLTt5WhU7E1mdi9qJ8zsAOBJQk/tq4TL1ecAfzSzHe4+AbVTMqm2SdbaTsG4/j4h+behbiT/ptWsmFlbwgjXwcCJ7r4ibvd6am87aOLtF01/u4UwqKRNwr26NmbWFdhMM2+nyDpCz/iFhO2TCaOn90HtBGEA1y7gTHffFW17ycx6AL82s0cI7ZRs6mWsndYn2dfUpdom64FuZmYJveN6t50uU9dfCeE+QqJhwLsNXJdGJboE+yRwNPB5d38noUhdbbfM3bdkuYq5NhhoSxj88UncA8LUsE+AQ1A7waf35BLFeiRVqJ0g/L7MiwvEMTOBHoTRwCXAoGhaZrxhhJHYifdDm4NU26QEaAPsn6Qc1OMzX8G4/p4FRpvZ4NiG6NLZcdG+ZimaSzyBMHDkHHefnqTYs8C+ZnZi3HGdgbNoHm03Fzg5yQNCgD6Z8CHQ3NsJ4J/Rc3HC9mJghbuXonaCMD3ucDNrnbB9FLCD0HN7ljAP+YLYTjNrCVwETHb38gaqa2OSaptMJATnsQnHjwPmRzNp9k6u54Dl+wPoQPjAfIdwb+ZsYB6wiIS5fs3pQVjkw4G7gNEJj35RmQLgDWA58CXCB+srhA+M/XL9HnLYdonzjJt9OxF6wC8TLld/lTCA676orS5XO+1up/OjNpkUfR6dTlicwoFfxpV7lHDl5X8IX5ifIATrI3L9HrLYLufHfS5dG/18YrptQhhAuAP4DmGw3B8IV2bOqlcdc91ITeFBuNfwJLCJcI/vaRImlTe3B7Ak+qVP9vhhXLnuwAPRB+Y2wuT5w3Jd/xy3XbVgrHba3QadCSODywi9k7eBS9RONdrpc9GXkDXR59FcwrS5FnFl2gG/JPSkdwAzgJNyXfcstkltn0WvpNsmQAvg+8BSwjSnt4Hz61tHpVAUERHJMd0zFhERyTEFYxERkRxTMBYREckxBWMREZEcUzAWERHJMQVjERGRHFMwFpG0mdnlZuZmNiRh+1Fmtt7M5phZz1zVTyTfKBiLSEaY2bHAi8CHwCnuvjbHVRLJGwrGIlJv0XrQkwjLwp7m7s0hQ5JIxigYi0i9mNlpwH+AN4Fid9+U4yqJ5B0FYxGpjzOA54DXgDPcfWuO6yOSlxSMRaQ+7gFWENJkbs91ZUTylYKxiNTH84RE6zfluiIi+axlrisgInnt24SUc7eZ2Q53vzvXFRLJRwrGIlIfDlwNtAF+EgXke3JcJ5G8o2AsIvXi7lVmdjnQGvhVFJD/mONqieQVBWMRqTd3rzSzsYQe8u/NrNzdH8x1vUTyhQZwiUhGuHsFcCEwEbjfzC7JcZVE8oa5e67rICIi0qypZywiIpJjCsYiIiI5pmAsIiKSYwrGIiIiOaZgLCIikmMKxiIiIjmmYCwiIpJjCsYiIiI59v8B3xETojiKeAEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def plot_k_vs_heterogeneity(k_values, heterogeneity_values):\n",
    "    \"\"\"\n",
    "    Given list of k-values and their heterogeneities, will make a plot\n",
    "    showing how heterogeneity varies with k.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(k_values, heterogeneity_values, linewidth=4)\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('K vs. Heterogeneity')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "\n",
    "all_centroids = {}\n",
    "all_cluster_assignment = {}\n",
    "heterogeneity_values = []\n",
    "seeds = [20000, 40000, 80000]\n",
    "k_list = [2, 10, 25, 50, 100]\n",
    "\n",
    "for k in k_list:\n",
    "    print(f'Running k = {k}')\n",
    "    heterogeneity = []\n",
    "    all_centroids[k], all_cluster_assignment[k] = kmeans_multiple_runs(tf_idf, k, maxiter=400,\n",
    "                                                                       seeds=seeds, verbose=True)\n",
    "    score = compute_heterogeneity(tf_idf, k, all_centroids[k], all_cluster_assignment[k])\n",
    "    heterogeneity_values.append(score)\n",
    "\n",
    "plot_k_vs_heterogeneity(k_list, heterogeneity_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sXShf7ZqEt3"
   },
   "source": [
    "## Visualize clusters of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ah5tJcZqEt3"
   },
   "source": [
    "Let's start visualizing some clustering results to see if we think the clustering makes sense.  We can use such visualizations to help us assess whether we have set K too large or too small for a given application.  Following the theme of this course, we will judge whether the clustering makes sense in the context of document analysis.\n",
    "\n",
    "What are we looking for in a good clustering of documents?\n",
    "* Documents in the same cluster should be similar.\n",
    "* Documents from different clusters should be less similar.\n",
    "\n",
    "So a bad clustering exhibits either of two symptoms:\n",
    "* Documents in a cluster have mixed content.\n",
    "* Documents with similar content are divided up and put into different clusters.\n",
    "\n",
    "To help visualize the clustering, we do the following:\n",
    "* Fetch nearest neighbors of each centroid from the set of documents assigned to that cluster. We will consider these documents as being representative of the cluster.\n",
    "* Print titles and first sentences of those nearest neighbors.\n",
    "* Print top 5 words that have highest tf-idf weights in each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NOYSYztKqEt3"
   },
   "outputs": [],
   "source": [
    "def visualize_document_clusters(wiki, tf_idf, centroids, cluster_assignment, k, words, \n",
    "                                display_docs=5):\n",
    "    \"\"\"\n",
    "    Given a set of clustered documents, prints information about the centroids including\n",
    "       - The title and starting sentence of the closest 5 points to each centroid\n",
    "       - The five words that are contained in the clusters documents with the highest TF-IDF.\n",
    "    \n",
    "    Parameters:  \n",
    "      - wiki: original dataframe\n",
    "      - tf_idf: data matrix containing TF-IDF vectors for each document\n",
    "      - centroids: A np.array of length k that contains the centroids for the clustering\n",
    "      - cluster_assignments: A np.array of length N that has the cluster assignments for each row\n",
    "      - k: What value of k is used\n",
    "      - words: List of words in the corpus (should match tf_idf)\n",
    "      - display_odcs: How many documents to show for each cluster (default 5)\n",
    "    \"\"\"\n",
    "    print('=' * 90)\n",
    "\n",
    "    # Visualize each cluster c\n",
    "    for c in range(k):\n",
    "        # Cluster heading\n",
    "        print(f'Cluster {c}  ({(cluster_assignment == c).sum()} docs)'),\n",
    "        # Print top 5 words with largest TF-IDF weights in the cluster\n",
    "        idx = centroids[c].argsort()[::-1]\n",
    "        for i in range(5): # Print each word along with the TF-IDF weight\n",
    "            print(f'{words[idx[i]]}:{centroids[c,idx[i]]:.3f}', end=' '),\n",
    "        print()\n",
    "        \n",
    "        if display_docs > 0:\n",
    "            print()\n",
    "            # Compute distances from the centroid to all data points in the cluster,\n",
    "            # and compute nearest neighbors of the centroids within the cluster.\n",
    "            distances = pairwise_distances(tf_idf, centroids[c].reshape(1, -1), metric='euclidean').flatten()\n",
    "            distances[cluster_assignment!=c] = float('inf') # remove non-members from consideration\n",
    "            nearest_neighbors = distances.argsort()\n",
    "            # For the nearest neighbors, print the title as well as first 180 characters of text.\n",
    "            # Wrap the text at 80-character mark.\n",
    "            for i in range(display_docs):\n",
    "                text = ' '.join(wiki.iloc[nearest_neighbors[i]]['text'].split(None, 25)[0:25])\n",
    "                print(f'* {wiki.iloc[nearest_neighbors[i]][\"name\"]:50s} {distances[nearest_neighbors[i]]:.5f}')\n",
    "                print(f'  {text[:90]}')\n",
    "                if len(text) > 90:\n",
    "                    print(f'  {text[90:180]}')\n",
    "                print()\n",
    "        print('=' * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Db4HxJaqEt5"
   },
   "source": [
    "Let us first look at the 2 cluster case (K=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oI9VSXJxqEt5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cluster 0  (2576 docs)\n",
      "she:0.071 her:0.051 for:0.038 was:0.034 as:0.033 \n",
      "\n",
      "* Natashia Williams                                  0.93245\n",
      "  natashia williamsblach born august 2 1978 is an american actress and former wonderbra camp\n",
      "  aign model who is perhaps best known for her role as shane\n",
      "\n",
      "* Bhama Srinivasan                                   0.94091\n",
      "  bhama srinivasan april 22 1935 is a mathematician known for her work in the representation\n",
      "   theory of finite groups her contributions were honored with the\n",
      "\n",
      "* Bette McLaurin                                     0.94135\n",
      "  bette mclaurin born c1929 is an africanamerican singer best known for her jazzinfluenced b\n",
      "  allad and rb performances in the 1950s two of her recordings i\n",
      "\n",
      "* Delores Brumfield                                  0.94150\n",
      "  delores brumfield white born may 26 1932 is a former utility infielderoutfielder who playe\n",
      "  d from 1947 through 1953 in the allamerican girls professional baseball league\n",
      "\n",
      "* Anne Harris (musician)                             0.94196\n",
      "  anne harris is a singer songwriter violinist recording artist and actor based in chicago i\n",
      "  llinois she has independently produced and released four studio albums on\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 1  (3331 docs)\n",
      "he:0.085 was:0.044 his:0.043 for:0.039 as:0.034 \n",
      "\n",
      "* Billy Bingham                                      0.94607\n",
      "  william laurence billy bingham mbe born 5 august 1931 is a former international footballer\n",
      "   and football manager who now works as a scout for english\n",
      "\n",
      "* Nicky Banger                                       0.94670\n",
      "  nicholas lee banger born 25 february 1971 is a retired english professional footballer he \n",
      "  is currently head of commercial operations with woking fcbanger was born\n",
      "\n",
      "* Wally Whitehurst                                   0.94812\n",
      "  walter richard whitehurst born april 11 1964 in shreveport louisiana is a former righthand\n",
      "  ed pitcher in major league baseball who played from 1989 to 1996\n",
      "\n",
      "* Steve Bruce                                        0.94948\n",
      "  stephen roger steve bruce born 31 december 1960 is an english football manager and former \n",
      "  player who is currently the manager at hull city born\n",
      "\n",
      "* Mel Charles                                        0.94999\n",
      "  melvyn mel charles born 14 may 1935 as melfyn charles is a welsh former international foot\n",
      "  baller he is the brother of john charles a legendary\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "visualize_document_clusters(wiki, tf_idf, all_centroids[k], all_cluster_assignment[k], k, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipEHA_0tqEt7"
   },
   "source": [
    "Both clusters have mixed content, although clearly cluster 0 are all women and cluster 1 are all men:\n",
    "\n",
    "It would be better if we sub-divided into more categories. So let us use more clusters. How about `K=10`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhpIgIF_qEt8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cluster 0  (1240 docs)\n",
      "she:0.113 her:0.090 for:0.040 was:0.036 as:0.032 \n",
      "\n",
      "* Bhama Srinivasan                                   0.91625\n",
      "  bhama srinivasan april 22 1935 is a mathematician known for her work in the representation\n",
      "   theory of finite groups her contributions were honored with the\n",
      "\n",
      "* Delores Brumfield                                  0.91828\n",
      "  delores brumfield white born may 26 1932 is a former utility infielderoutfielder who playe\n",
      "  d from 1947 through 1953 in the allamerican girls professional baseball league\n",
      "\n",
      "* Bette McLaurin                                     0.92113\n",
      "  bette mclaurin born c1929 is an africanamerican singer best known for her jazzinfluenced b\n",
      "  allad and rb performances in the 1950s two of her recordings i\n",
      "\n",
      "* Kate Walsh (singer)                                0.92510\n",
      "  kate walsh born february 20 1983 is an english singer from burnhamoncrouch essex a graduat\n",
      "  e of the brighton institute of modern music her first album\n",
      "\n",
      "* Gila Golan                                         0.92615\n",
      "  gila golan hebrew born 1940 is a polishborn israeli former fashion model and actressgolan \n",
      "  was born in krakw poland around 1940 her exact birthday is\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 1  (812 docs)\n",
      "he:0.083 his:0.046 was:0.042 for:0.042 coach:0.040 \n",
      "\n",
      "* Mike Houston (American football)                   0.92862\n",
      "  mike houston born c 1971 is an american college football coach currently serving as head c\n",
      "  oach of the citadel bulldogs football team he was named\n",
      "\n",
      "* Tyrone Wheatley                                    0.93469\n",
      "  tyrone anthony wheatley born january 19 1972 is the running backs coach of michigan and a \n",
      "  former professional american football player who played 10 seasons\n",
      "\n",
      "* Wally Whitehurst                                   0.93534\n",
      "  walter richard whitehurst born april 11 1964 in shreveport louisiana is a former righthand\n",
      "  ed pitcher in major league baseball who played from 1989 to 1996\n",
      "\n",
      "* Radojko Avramovi%C4%87                             0.93546\n",
      "  radojko raddy avramovi serbian cyrillic born 29 november 1949 is a serbian football coach \n",
      "  who played for yugoslavia as a goalkeeper he is currently coach\n",
      "\n",
      "* Sixto Lezcano                                      0.93768\n",
      "  sixto joaquin lezcano curras born november 28 1953 in arecibo puerto rico is a retired bas\n",
      "  eball player who played for 12 seasons as an outfielder\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 2  (157 docs)\n",
      "he:0.057 university:0.042 director:0.042 for:0.035 as:0.034 \n",
      "\n",
      "* George Yip                                         0.94135\n",
      "  george yip is a research specialist in global strategy and marketing and professor of mana\n",
      "  gement and codirector centre on china innovation at china europe international\n",
      "\n",
      "* Vittorio Corbo                                     0.94556\n",
      "  vittorio corbo lioi born 22 march 1943 is a former governor of the central bank of chile w\n",
      "  ho held the post since may 2003 until\n",
      "\n",
      "* Tim Jenkinson                                      0.94629\n",
      "  tim jenkinson is professor of finance at the sad business school university of oxford his \n",
      "  research is on initial public offerings in particular the analysis\n",
      "\n",
      "* Gerald J. Lynch                                    0.95131\n",
      "  jerry lynch is the interim dean of the krannert school of management and professor of econ\n",
      "  omics at purdue university lynch earned his bachelor of arts\n",
      "\n",
      "* David Gompert                                      0.95305\n",
      "  david charles gompert born october 6 1945 officially joined the office of the director of \n",
      "  national intelligence on november 10 2009 as the principal deputy\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 3  (868 docs)\n",
      "he:0.077 his:0.050 was:0.048 for:0.045 league:0.038 \n",
      "\n",
      "* Tony Smith (footballer, born 1957)                 0.93386\n",
      "  anthony tony smith born 20 february 1957 is a former footballer who played as a central de\n",
      "  fender in the football league in the 1970s and\n",
      "\n",
      "* Steve Bruce                                        0.93631\n",
      "  stephen roger steve bruce born 31 december 1960 is an english football manager and former \n",
      "  player who is currently the manager at hull city born\n",
      "\n",
      "* Mark West (footballer)                             0.93846\n",
      "  mark west born 3 april 1973 is a former australian rules footballer who played with the we\n",
      "  stern bulldogs in the australian football league afl during\n",
      "\n",
      "* Mel Charles                                        0.93870\n",
      "  melvyn mel charles born 14 may 1935 as melfyn charles is a welsh former international foot\n",
      "  baller he is the brother of john charles a legendary\n",
      "\n",
      "* Brad Fisher                                        0.93906\n",
      "  bradley fisher born 9 april 1984 is an australian rules footballer in the australian footb\n",
      "  all leaguefisher was recruited in the 2002 afl draft from east\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 4  (886 docs)\n",
      "he:0.076 his:0.052 music:0.049 with:0.036 for:0.035 \n",
      "\n",
      "* Wilson McLean                                      0.93853\n",
      "  wilson mclean born 1937 is a scottish illustrator and artist he has illustrated primarily \n",
      "  in the field of advertising but has also provided cover art\n",
      "\n",
      "* David Oei                                          0.94159\n",
      "  david oei chinese name pinyin hung jln surname pronounced wee is hong kongborn american cl\n",
      "  assical pianist b 1950 in hong kongoei was born in hong\n",
      "\n",
      "* Graham Ord                                         0.94272\n",
      "  graham ord born 22 march 1961 is an english musician and songwriter he has garnered respec\n",
      "  t internationally as a fine musician and engaging communicator his\n",
      "\n",
      "* Robert Scott Thompson                              0.94527\n",
      "  robert scott thompson born 1959 california is a composer of ambient instrumental and elect\n",
      "  roacoustic music he earned the bmus degree from the university of oregon\n",
      "\n",
      "* John Abram                                         0.94718\n",
      "  john abram born 1959 is an anglocanadian composer best known for his work with electroacou\n",
      "  stic musicborn in england abram became interested in music when he\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 5  (218 docs)\n",
      "he:0.067 world:0.058 championships:0.056 at:0.044 won:0.039 \n",
      "\n",
      "* Christopher Hedquist                               0.92764\n",
      "  christopher hedquist born june 4 1980 salt lake city utah is an american skeleton racer in\n",
      "   2004 he won the overall european cup becoming the\n",
      "\n",
      "* Yang Yansheng                                      0.92860\n",
      "  yang yansheng simplified chinese born 5 january 1988 is a chinese pole vaulter he is the c\n",
      "  hinese record holder for the event both indoors and\n",
      "\n",
      "* Marty Krulee                                       0.92961\n",
      "  marty krulee born november 4 1956 is an american world class track and field athlete prima\n",
      "  rily known for running sprint races while never achieving outstanding\n",
      "\n",
      "* Anastasiya Rabchenyuk                              0.92964\n",
      "  anastasiya rabchenyuk ukrainian born 14 september 1983 is a ukrainian hurdler she was born\n",
      "   in ternivka at the time in the ukrainian ssr of the\n",
      "\n",
      "* Yordanis Garc%C3%ADa                               0.93031\n",
      "  yordanis garca barrisonte spanish pronunciation oranis arsia arisonte born november 21 198\n",
      "  8 in san juan y martnez pinar del ro is a male decathlete from\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 6  (194 docs)\n",
      "new:0.065 york:0.062 he:0.060 for:0.037 his:0.035 \n",
      "\n",
      "* William M. Skretny                                 0.93837\n",
      "  william marion skretny born 1945 is a united states federal judgeborn in buffalo new york \n",
      "  skretny received an ab from canisius college in 1966 a\n",
      "\n",
      "* Tom Moody (artist)                                 0.94421\n",
      "  tom moody is a visual artist critic and blogger based in new york city he began his career\n",
      "   as a painter using traditional materials but\n",
      "\n",
      "* Christopher Lehmann-Haupt                          0.94494\n",
      "  christopher lehmannhaupt born 1934 is an american journalist critic and novelist he began \n",
      "  as an editor for various new york city publishing houses among them\n",
      "\n",
      "* Robert J. Gaffney                                  0.94593\n",
      "  robert j gaffney born 19441945 was the sixth county executive of suffolk county new york f\n",
      "  irst elected in 1991 he served through 2003 he was\n",
      "\n",
      "* Francis Edwards Peters                             0.94831\n",
      "  francis edward peters born june 23 1927 new york city who generally publishes as fe peters\n",
      "   is professor emeritus of history religion and middle eastern\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 7  (115 docs)\n",
      "opera:0.109 she:0.062 de:0.040 at:0.040 he:0.039 \n",
      "\n",
      "* Susannah Waters                                    0.90292\n",
      "  susannah waters is a british writer and director born in kent england she attended both be\n",
      "  nnington college in america and the guildhall school of music\n",
      "\n",
      "* Martina Arroyo                                     0.90560\n",
      "  martina arroyo born february 2 1937 is an american operatic soprano who had a major intern\n",
      "  ational opera career from the 1960s through the 1980s she\n",
      "\n",
      "* Graham Clark (tenor)                               0.91019\n",
      "  graham clark born in 1941 lancashire england is an english opera tenor mainly known for hi\n",
      "  s character roles like loge das rheingold mime siegfried and\n",
      "\n",
      "* Vinson Cole                                        0.91056\n",
      "  vinson cole born november 21 1950 is an american operatic tenora native of kansas city the\n",
      "   tenor studied at the university of missouri kansas city\n",
      "\n",
      "* Gloria Lane                                        0.91229\n",
      "  gloria lane born june 6 1930 trenton new jersey is an american operatic mezzosoprano who h\n",
      "  ad an active international performance career from 1949 to 1976\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 8  (1119 docs)\n",
      "he:0.084 was:0.049 as:0.038 for:0.036 university:0.036 \n",
      "\n",
      "* Doug Naysmith                                      0.94280\n",
      "  john douglas naysmith born 1 april 1941 is a british labour cooperative politician who was\n",
      "   the member of parliament mp for bristol north west from\n",
      "\n",
      "* Robert Worcester                                   0.94482\n",
      "  sir robert milton worcester kbe dl born 21 december 1933 is the founder of mori market opi\n",
      "  nion research international ltd and a member and contributor\n",
      "\n",
      "* Bill Clinton                                       0.94532\n",
      "  william jefferson bill clinton born william jefferson blythe iii august 19 1946 is an amer\n",
      "  ican politician who served from 1993 to 2001 as the 42nd\n",
      "\n",
      "* Hugo Parr                                          0.94540\n",
      "  hugo ragnar parr born 23 april 1947 is a norwegian physicist civil servant and politician \n",
      "  for the labour partyhe was born in asker he took\n",
      "\n",
      "* Elijah Anderson                                    0.94638\n",
      "  elijah anderson is an american sociologist he holds the william k lanman jr professorship \n",
      "  in sociology at yale university where he teaches and directs the\n",
      "\n",
      "==========================================================================================\n",
      "Cluster 9  (298 docs)\n",
      "she:0.080 series:0.059 film:0.053 television:0.044 as:0.039 \n",
      "\n",
      "* Natashia Williams                                  0.91607\n",
      "  natashia williamsblach born august 2 1978 is an american actress and former wonderbra camp\n",
      "  aign model who is perhaps best known for her role as shane\n",
      "\n",
      "* Sarah Lancashire                                   0.93031\n",
      "  sarah lancashire born 10 october 1964 is an english television film and theatre actress wh\n",
      "  o has also presented and directed for television lancashire trained at\n",
      "\n",
      "* Andrea Guasch                                      0.93223\n",
      "  andrea guasch born december 20 1990 is a spanish actress singer and dancer she started her\n",
      "   artistic career when she was just 2 years old\n",
      "\n",
      "* Sarah Michelle Gellar                              0.93389\n",
      "  sarah michelle prinze ne gellar born april 14 1977 is an american actress and producer aft\n",
      "  er being spotted by an agent at the age of\n",
      "\n",
      "* Malin %C3%85kerman                                 0.93460\n",
      "  malin maria kerman swedish pronunciation mln okrman born may 12 1978 is a swedish canadian\n",
      "   actress model and singer she is best known as an\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "visualize_document_clusters(wiki, tf_idf, all_centroids[k], all_cluster_assignment[k], k, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFEmQgrkqEt9"
   },
   "source": [
    "We no longer have the clear split between men and women. Cluters 0 and 2 appear to be still mixed, but others are quite consistent in content.\n",
    "* Cluster 0: notable women\n",
    "* Cluster 1: baseball players\n",
    "* Cluster 2: researchers, professors\n",
    "* Cluster 3: football(soccer)\n",
    "* Cluster 4: musicians, singers, song writers\n",
    "* Cluster 5: golfers\n",
    "* Cluster 6: painters, scultpers, artists\n",
    "* Cluster 7: orchestral musicians, conductors\n",
    "* Cluster 8: politicians, political personel\n",
    "* Cluster 9: film directors|\n",
    "\n",
    "Clusters are now more pure, but some are qualitatively \"bigger\" than others. For instance, the category of scholars is more general than the category of film directors. Increasing the number of clusters may split larger clusters. Another way to look at the size of cluster is to count the number of articles in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAiddEjHqEt-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1240,  812,  157,  868,  886,  218,  194,  115, 1119,  298],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(all_cluster_assignment[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cSpQE1UqEuB"
   },
   "source": [
    "There appears to be at least some connection between the topical consistency of a cluster and the number of its member data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q4u9EhVqEuB"
   },
   "source": [
    "Let us visualize the case for K=25. For the sake of brevity, we do not print the content of documents. It turns out that the top words with highest TF-IDF weights in each cluster are representative of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uE6Ih-buqEuC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cluster 0  (297 docs)\n",
      "band:0.078 album:0.074 he:0.053 with:0.045 his:0.044 \n",
      "==========================================================================================\n",
      "Cluster 1  (169 docs)\n",
      "he:0.084 bbc:0.052 as:0.047 his:0.043 for:0.041 \n",
      "==========================================================================================\n",
      "Cluster 2  (125 docs)\n",
      "art:0.093 she:0.080 museum:0.046 her:0.037 at:0.037 \n",
      "==========================================================================================\n",
      "Cluster 3  (112 docs)\n",
      "he:0.075 was:0.043 medical:0.038 as:0.037 chief:0.037 \n",
      "==========================================================================================\n",
      "Cluster 4  (55 docs)\n",
      "language:0.067 he:0.048 linguistics:0.043 university:0.040 his:0.037 \n",
      "==========================================================================================\n",
      "Cluster 5  (396 docs)\n",
      "she:0.144 her:0.078 was:0.047 at:0.041 for:0.032 \n",
      "==========================================================================================\n",
      "Cluster 6  (188 docs)\n",
      "he:0.068 as:0.044 law:0.043 for:0.042 on:0.036 \n",
      "==========================================================================================\n",
      "Cluster 7  (33 docs)\n",
      "he:0.077 league:0.058 france:0.057 for:0.056 club:0.055 \n",
      "==========================================================================================\n",
      "Cluster 8  (148 docs)\n",
      "he:0.057 his:0.053 music:0.047 as:0.039 dj:0.036 \n",
      "==========================================================================================\n",
      "Cluster 9  (951 docs)\n",
      "he:0.078 university:0.049 at:0.038 was:0.038 research:0.036 \n",
      "==========================================================================================\n",
      "Cluster 10  (265 docs)\n",
      "music:0.100 he:0.060 for:0.044 his:0.042 with:0.041 \n",
      "==========================================================================================\n",
      "Cluster 11  (183 docs)\n",
      "he:0.097 club:0.061 his:0.046 for:0.046 was:0.043 \n",
      "==========================================================================================\n",
      "Cluster 12  (86 docs)\n",
      "opera:0.185 she:0.084 her:0.044 with:0.040 at:0.038 \n",
      "==========================================================================================\n",
      "Cluster 13  (21 docs)\n",
      "he:0.075 his:0.059 climbing:0.050 everest:0.048 yorkshire:0.046 \n",
      "==========================================================================================\n",
      "Cluster 14  (206 docs)\n",
      "minister:0.102 party:0.068 was:0.058 he:0.057 she:0.054 \n",
      "==========================================================================================\n",
      "Cluster 15  (401 docs)\n",
      "he:0.084 his:0.056 was:0.040 championship:0.038 at:0.038 \n",
      "==========================================================================================\n",
      "Cluster 16  (777 docs)\n",
      "she:0.103 her:0.101 film:0.054 for:0.041 as:0.034 \n",
      "==========================================================================================\n",
      "Cluster 17  (200 docs)\n",
      "he:0.083 was:0.068 election:0.059 party:0.049 as:0.043 \n",
      "==========================================================================================\n",
      "Cluster 18  (21 docs)\n",
      "china:0.102 chinese:0.096 healthcare:0.039 he:0.039 chinas:0.037 \n",
      "==========================================================================================\n",
      "Cluster 19  (251 docs)\n",
      "he:0.076 was:0.050 as:0.040 for:0.038 district:0.038 \n",
      "==========================================================================================\n",
      "Cluster 20  (211 docs)\n",
      "he:0.061 his:0.045 music:0.042 guitar:0.040 with:0.035 \n",
      "==========================================================================================\n",
      "Cluster 21  (300 docs)\n",
      "he:0.084 was:0.052 played:0.052 football:0.049 basketball:0.048 \n",
      "==========================================================================================\n",
      "Cluster 22  (162 docs)\n",
      "he:0.087 league:0.080 season:0.061 football:0.059 his:0.057 \n",
      "==========================================================================================\n",
      "Cluster 23  (179 docs)\n",
      "league:0.128 baseball:0.113 he:0.091 major:0.067 games:0.059 \n",
      "==========================================================================================\n",
      "Cluster 24  (170 docs)\n",
      "he:0.070 for:0.051 sports:0.045 radio:0.045 on:0.045 \n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "k = 25\n",
    "visualize_document_clusters(wiki, tf_idf, all_centroids[k], all_cluster_assignment[k], k,\n",
    "                            words, display_docs=0) # turn off text for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zrrQ7yqqEuD"
   },
   "source": [
    "Looking at the representative examples and top words, we classify each cluster as follows. Notice the bolded items, which indicate the appearance of a new theme.\n",
    "* Cluster 0: **British labor party**\n",
    "* Cluster 1: **Bishops**\n",
    "* Cluster 2: **danish CEOs**\n",
    "* Cluster 3: baseball\n",
    "* Cluster 4: politicials\n",
    "* Cluster 5: **psychology researchers**\n",
    "* Cluster 6: **medical researchers**\n",
    "* Cluster 7: **republican politicians**\n",
    "* Cluster 8: football(soccer)\n",
    "* Cluster 9: **prime ministers**\n",
    "* Cluster 10: golfers\n",
    "* Cluster 11: coaches\n",
    "* Cluster 12: **lawers**\n",
    "* Cluster 13: researchers, professors\n",
    "* Cluster 14: writers\n",
    "* Cluster 15: artists, museaum workers\n",
    "* Cluster 16: film directors\n",
    "* Cluster 17: musicians\n",
    "* Cluster 18: **airforce commanders**\n",
    "* Cluster 19: orchestral musicians\n",
    "* Cluster 20: *unclear*\n",
    "* Cluster 21: *unclear*\n",
    "* Cluster 22: *unclear*\n",
    "* Cluster 23: politicians\n",
    "* Cluster 24: **hockey players**\n",
    "\n",
    "Indeed, increasing K achieved the desired effect of breaking up large clusters.  Depending on the application, this may or may not be preferable to the K=10 analysis.\n",
    "\n",
    "Let's take it to the extreme and set K=100. We have a suspicion that this value is too large. Let us look at the top words from each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xx-YopxTqEuE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Cluster 0  (236 docs)\n",
      "she:0.099 her:0.091 music:0.045 for:0.044 with:0.036 \n",
      "==========================================================================================\n",
      "Cluster 1  (210 docs)\n",
      "coach:0.110 he:0.082 team:0.050 football:0.047 was:0.047 \n",
      "==========================================================================================\n",
      "Cluster 2  (14 docs)\n",
      "director:0.065 he:0.063 mr:0.058 hooker:0.050 jindal:0.050 \n",
      "==========================================================================================\n",
      "Cluster 3  (140 docs)\n",
      "football:0.082 he:0.078 australian:0.077 afl:0.060 his:0.056 \n",
      "==========================================================================================\n",
      "Cluster 4  (188 docs)\n",
      "music:0.115 he:0.086 his:0.053 with:0.046 has:0.038 \n",
      "==========================================================================================\n",
      "Cluster 5  (90 docs)\n",
      "championships:0.084 world:0.077 he:0.072 metres:0.056 at:0.053 \n",
      "==========================================================================================\n",
      "Cluster 6  (9 docs)\n",
      "yoga:0.236 iskcon:0.089 he:0.065 new:0.063 swami:0.062 \n",
      "==========================================================================================\n",
      "Cluster 7  (47 docs)\n",
      "opera:0.227 she:0.074 at:0.042 with:0.041 la:0.040 \n",
      "==========================================================================================\n",
      "Cluster 8  (110 docs)\n",
      "he:0.087 chairman:0.069 board:0.058 was:0.051 member:0.038 \n",
      "==========================================================================================\n",
      "Cluster 9  (98 docs)\n",
      "she:0.117 series:0.075 miss:0.073 her:0.065 role:0.059 \n",
      "==========================================================================================\n",
      "Cluster 10  (48 docs)\n",
      "law:0.268 he:0.073 legal:0.062 school:0.054 court:0.052 \n",
      "==========================================================================================\n",
      "Cluster 11  (64 docs)\n",
      "japanese:0.070 he:0.060 his:0.041 war:0.036 for:0.035 \n",
      "==========================================================================================\n",
      "Cluster 12  (105 docs)\n",
      "law:0.072 he:0.071 federal:0.046 was:0.046 as:0.044 \n",
      "==========================================================================================\n",
      "Cluster 13  (67 docs)\n",
      "minister:0.086 she:0.070 was:0.050 finance:0.048 he:0.048 \n",
      "==========================================================================================\n",
      "Cluster 14  (94 docs)\n",
      "she:0.192 her:0.056 was:0.051 for:0.047 on:0.039 \n",
      "==========================================================================================\n",
      "Cluster 15  (24 docs)\n",
      "football:0.148 bailey:0.079 virginia:0.064 he:0.059 was:0.044 \n",
      "==========================================================================================\n",
      "Cluster 16  (45 docs)\n",
      "he:0.071 republican:0.062 senate:0.062 district:0.061 seat:0.056 \n",
      "==========================================================================================\n",
      "Cluster 17  (39 docs)\n",
      "sox:0.142 league:0.103 red:0.099 baseball:0.097 he:0.094 \n",
      "==========================================================================================\n",
      "Cluster 18  (23 docs)\n",
      "bush:0.080 president:0.073 service:0.071 he:0.058 corps:0.056 \n",
      "==========================================================================================\n",
      "Cluster 19  (21 docs)\n",
      "manitoba:0.154 he:0.084 was:0.059 election:0.051 provincial:0.049 \n",
      "==========================================================================================\n",
      "Cluster 20  (38 docs)\n",
      "he:0.083 was:0.072 election:0.052 as:0.044 mayor:0.044 \n",
      "==========================================================================================\n",
      "Cluster 21  (26 docs)\n",
      "canadian:0.082 curling:0.075 ontario:0.074 championships:0.070 team:0.060 \n",
      "==========================================================================================\n",
      "Cluster 22  (60 docs)\n",
      "he:0.077 connecticut:0.050 was:0.046 his:0.040 hall:0.039 \n",
      "==========================================================================================\n",
      "Cluster 23  (83 docs)\n",
      "band:0.118 he:0.057 album:0.047 bands:0.041 with:0.041 \n",
      "==========================================================================================\n",
      "Cluster 24  (138 docs)\n",
      "he:0.068 for:0.043 university:0.040 his:0.040 on:0.037 \n",
      "==========================================================================================\n",
      "Cluster 25  (152 docs)\n",
      "he:0.098 cup:0.075 league:0.067 for:0.057 his:0.056 \n",
      "==========================================================================================\n",
      "Cluster 26  (25 docs)\n",
      "he:0.087 comedy:0.081 show:0.063 appeared:0.056 television:0.054 \n",
      "==========================================================================================\n",
      "Cluster 27  (26 docs)\n",
      "was:0.057 he:0.053 court:0.048 murder:0.041 that:0.040 \n",
      "==========================================================================================\n",
      "Cluster 28  (67 docs)\n",
      "church:0.100 he:0.067 lds:0.051 was:0.046 as:0.041 \n",
      "==========================================================================================\n",
      "Cluster 29  (71 docs)\n",
      "hockey:0.207 nhl:0.099 he:0.073 ice:0.069 season:0.068 \n",
      "==========================================================================================\n",
      "Cluster 30  (38 docs)\n",
      "health:0.154 he:0.069 medical:0.046 was:0.044 hospital:0.043 \n",
      "==========================================================================================\n",
      "Cluster 31  (66 docs)\n",
      "for:0.057 animation:0.052 he:0.051 york:0.045 new:0.044 \n",
      "==========================================================================================\n",
      "Cluster 32  (20 docs)\n",
      "army:0.105 singh:0.091 he:0.072 general:0.067 was:0.060 \n",
      "==========================================================================================\n",
      "Cluster 33  (51 docs)\n",
      "poetry:0.177 poems:0.095 poet:0.063 published:0.055 she:0.050 \n",
      "==========================================================================================\n",
      "Cluster 34  (29 docs)\n",
      "he:0.060 bank:0.058 dublin:0.046 was:0.044 executive:0.041 \n",
      "==========================================================================================\n",
      "Cluster 35  (45 docs)\n",
      "he:0.065 was:0.046 his:0.044 that:0.039 on:0.037 \n",
      "==========================================================================================\n",
      "Cluster 36  (14 docs)\n",
      "triathlon:0.105 he:0.067 olympics:0.067 competed:0.062 world:0.053 \n",
      "==========================================================================================\n",
      "Cluster 37  (115 docs)\n",
      "league:0.115 he:0.097 baseball:0.096 season:0.069 major:0.064 \n",
      "==========================================================================================\n",
      "Cluster 38  (25 docs)\n",
      "he:0.085 his:0.066 leslie:0.050 was:0.039 on:0.037 \n",
      "==========================================================================================\n",
      "Cluster 39  (19 docs)\n",
      "khan:0.115 open:0.085 he:0.085 squash:0.061 title:0.048 \n",
      "==========================================================================================\n",
      "Cluster 40  (22 docs)\n",
      "billion:0.056 he:0.043 china:0.040 saudi:0.034 business:0.033 \n",
      "==========================================================================================\n",
      "Cluster 41  (3 docs)\n",
      "piatt:0.233 weather:0.190 fiber:0.101 tampa:0.100 them:0.095 \n",
      "==========================================================================================\n",
      "Cluster 42  (37 docs)\n",
      "chamber:0.109 orchestra:0.106 music:0.097 he:0.063 harpsichord:0.060 \n",
      "==========================================================================================\n",
      "Cluster 43  (31 docs)\n",
      "piano:0.178 he:0.063 music:0.060 his:0.051 symphony:0.050 \n",
      "==========================================================================================\n",
      "Cluster 44  (16 docs)\n",
      "rabbi:0.203 green:0.093 jewish:0.084 he:0.068 religious:0.049 \n",
      "==========================================================================================\n",
      "Cluster 45  (30 docs)\n",
      "votes:0.068 jersey:0.065 he:0.062 assembly:0.051 republican:0.050 \n",
      "==========================================================================================\n",
      "Cluster 46  (14 docs)\n",
      "software:0.081 he:0.076 his:0.046 computational:0.044 zend:0.043 \n",
      "==========================================================================================\n",
      "Cluster 47  (15 docs)\n",
      "scott:0.157 he:0.051 mcclure:0.049 pooh:0.048 2003s2003:0.044 \n",
      "==========================================================================================\n",
      "Cluster 48  (23 docs)\n",
      "guitar:0.120 duncan:0.088 he:0.055 with:0.041 his:0.040 \n",
      "==========================================================================================\n",
      "Cluster 49  (69 docs)\n",
      "he:0.074 physics:0.067 research:0.048 university:0.042 at:0.038 \n",
      "==========================================================================================\n",
      "Cluster 50  (82 docs)\n",
      "university:0.072 research:0.072 science:0.060 chemistry:0.057 professor:0.056 \n",
      "==========================================================================================\n",
      "Cluster 51  (1 docs)\n",
      "todori:0.629 ivica:0.349 agrokor:0.279 croatian:0.150 zagreb:0.146 \n",
      "==========================================================================================\n",
      "Cluster 52  (312 docs)\n",
      "he:0.085 university:0.062 research:0.046 at:0.045 his:0.042 \n",
      "==========================================================================================\n",
      "Cluster 53  (30 docs)\n",
      "nfl:0.141 football:0.073 he:0.070 basketball:0.067 for:0.053 \n",
      "==========================================================================================\n",
      "Cluster 54  (71 docs)\n",
      "art:0.149 her:0.083 she:0.071 work:0.045 museum:0.040 \n",
      "==========================================================================================\n",
      "Cluster 55  (56 docs)\n",
      "governor:0.085 election:0.074 he:0.072 party:0.061 was:0.058 \n",
      "==========================================================================================\n",
      "Cluster 56  (6 docs)\n",
      "keith:0.190 acappella:0.122 willis:0.098 scharf:0.064 linux:0.060 \n",
      "==========================================================================================\n",
      "Cluster 57  (49 docs)\n",
      "theatre:0.197 directed:0.053 play:0.047 he:0.046 at:0.045 \n",
      "==========================================================================================\n",
      "Cluster 58  (7 docs)\n",
      "ski:0.344 popangelov:0.087 alhajuj:0.073 bulgarian:0.071 winter:0.071 \n",
      "==========================================================================================\n",
      "Cluster 59  (54 docs)\n",
      "he:0.089 league:0.050 his:0.049 giants:0.047 was:0.046 \n",
      "==========================================================================================\n",
      "Cluster 60  (186 docs)\n",
      "he:0.096 was:0.062 as:0.051 from:0.046 minister:0.039 \n",
      "==========================================================================================\n",
      "Cluster 61  (27 docs)\n",
      "jazz:0.084 he:0.057 with:0.051 his:0.049 trumpet:0.035 \n",
      "==========================================================================================\n",
      "Cluster 62  (31 docs)\n",
      "campaign:0.096 he:0.050 for:0.044 was:0.043 as:0.040 \n",
      "==========================================================================================\n",
      "Cluster 63  (89 docs)\n",
      "art:0.162 museum:0.105 gallery:0.064 he:0.059 his:0.055 \n",
      "==========================================================================================\n",
      "Cluster 64  (13 docs)\n",
      "rt:0.138 irish:0.102 radio:0.086 he:0.068 bridgeman:0.061 \n",
      "==========================================================================================\n",
      "Cluster 65  (2 docs)\n",
      "ruiz:0.263 hjuler:0.201 baer:0.181 mama:0.176 kommissar:0.134 \n",
      "==========================================================================================\n",
      "Cluster 66  (48 docs)\n",
      "he:0.081 justice:0.063 turkish:0.048 was:0.047 as:0.041 \n",
      "==========================================================================================\n",
      "Cluster 67  (28 docs)\n",
      "orchestra:0.240 conductor:0.146 symphony:0.117 music:0.108 philharmonic:0.102 \n",
      "==========================================================================================\n",
      "Cluster 68  (28 docs)\n",
      "cancer:0.099 clinical:0.066 medical:0.066 research:0.061 medicine:0.054 \n",
      "==========================================================================================\n",
      "Cluster 69  (20 docs)\n",
      "buddhist:0.094 oxford:0.079 tibetan:0.071 he:0.062 buddhism:0.052 \n",
      "==========================================================================================\n",
      "Cluster 70  (5 docs)\n",
      "forsberg:0.085 tv:0.083 hearing:0.080 voice:0.076 reffett:0.076 \n",
      "==========================================================================================\n",
      "Cluster 71  (28 docs)\n",
      "winnipeg:0.066 he:0.065 manitoba:0.063 mayor:0.063 committee:0.063 \n",
      "==========================================================================================\n",
      "Cluster 72  (21 docs)\n",
      "comic:0.109 comics:0.083 san:0.067 francisco:0.064 he:0.054 \n",
      "==========================================================================================\n",
      "Cluster 73  (20 docs)\n",
      "poker:0.270 event:0.100 wsop:0.089 tournament:0.089 she:0.066 \n",
      "==========================================================================================\n",
      "Cluster 74  (54 docs)\n",
      "basketball:0.122 nba:0.110 points:0.102 he:0.079 rebounds:0.059 \n",
      "==========================================================================================\n",
      "Cluster 75  (23 docs)\n",
      "she:0.174 baseball:0.174 her:0.116 league:0.115 girls:0.086 \n",
      "==========================================================================================\n",
      "Cluster 76  (9 docs)\n",
      "irving:0.083 publishing:0.075 bonnycastle:0.074 dick:0.066 nyholm:0.065 \n",
      "==========================================================================================\n",
      "Cluster 77  (40 docs)\n",
      "was:0.068 he:0.060 charges:0.057 on:0.047 counts:0.041 \n",
      "==========================================================================================\n",
      "Cluster 78  (152 docs)\n",
      "he:0.096 club:0.062 season:0.056 his:0.053 for:0.052 \n",
      "==========================================================================================\n",
      "Cluster 79  (36 docs)\n",
      "he:0.126 german:0.111 his:0.057 was:0.046 for:0.037 \n",
      "==========================================================================================\n",
      "Cluster 80  (136 docs)\n",
      "she:0.225 her:0.128 was:0.040 at:0.035 with:0.032 \n",
      "==========================================================================================\n",
      "Cluster 81  (6 docs)\n",
      "hearst:0.125 leech:0.118 meter:0.107 hurdles:0.090 martel:0.088 \n",
      "==========================================================================================\n",
      "Cluster 82  (101 docs)\n",
      "her:0.164 she:0.140 was:0.044 for:0.034 fashion:0.033 \n",
      "==========================================================================================\n",
      "Cluster 83  (29 docs)\n",
      "he:0.097 his:0.061 speedway:0.057 for:0.055 racing:0.052 \n",
      "==========================================================================================\n",
      "Cluster 84  (175 docs)\n",
      "film:0.176 he:0.052 films:0.051 festival:0.045 for:0.043 \n",
      "==========================================================================================\n",
      "Cluster 85  (27 docs)\n",
      "voice:0.088 series:0.072 he:0.063 television:0.046 roles:0.045 \n",
      "==========================================================================================\n",
      "Cluster 86  (75 docs)\n",
      "minister:0.117 she:0.085 election:0.072 was:0.070 party:0.068 \n",
      "==========================================================================================\n",
      "Cluster 87  (11 docs)\n",
      "he:0.067 guest:0.060 elevator:0.056 film:0.054 television:0.053 \n",
      "==========================================================================================\n",
      "Cluster 88  (28 docs)\n",
      "hop:0.079 hip:0.077 he:0.063 wwe:0.061 wrestling:0.054 \n",
      "==========================================================================================\n",
      "Cluster 89  (34 docs)\n",
      "song:0.159 eurovision:0.054 band:0.050 on:0.044 with:0.039 \n",
      "==========================================================================================\n",
      "Cluster 90  (296 docs)\n",
      "album:0.083 her:0.079 she:0.071 music:0.054 released:0.044 \n",
      "==========================================================================================\n",
      "Cluster 91  (27 docs)\n",
      "air:0.181 force:0.094 commander:0.073 command:0.072 he:0.069 \n",
      "==========================================================================================\n",
      "Cluster 92  (136 docs)\n",
      "she:0.070 novel:0.058 her:0.055 for:0.047 published:0.045 \n",
      "==========================================================================================\n",
      "Cluster 93  (83 docs)\n",
      "he:0.071 new:0.065 york:0.048 for:0.046 his:0.037 \n",
      "==========================================================================================\n",
      "Cluster 94  (18 docs)\n",
      "parish:0.145 he:0.055 was:0.045 his:0.039 greenberg:0.038 \n",
      "==========================================================================================\n",
      "Cluster 95  (27 docs)\n",
      "league:0.113 runs:0.084 baseball:0.076 he:0.072 game:0.069 \n",
      "==========================================================================================\n",
      "Cluster 96  (65 docs)\n",
      "he:0.059 band:0.058 with:0.045 on:0.039 show:0.038 \n",
      "==========================================================================================\n",
      "Cluster 97  (14 docs)\n",
      "rao:0.113 film:0.087 he:0.074 indian:0.072 telugu:0.071 \n",
      "==========================================================================================\n",
      "Cluster 98  (42 docs)\n",
      "tour:0.201 golf:0.190 pga:0.177 he:0.088 his:0.061 \n",
      "==========================================================================================\n",
      "Cluster 99  (9 docs)\n",
      "theory:0.099 relativity:0.085 that:0.075 ortiz:0.065 he:0.062 \n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "k=100\n",
    "visualize_document_clusters(wiki, tf_idf, all_centroids[k], all_cluster_assignment[k], k,\n",
    "                            words, display_docs=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZgU3lAXqEuF"
   },
   "source": [
    "**A high value of K encourages pure clusters, but we cannot keep increasing K. For large enough K, related documents end up going to different clusters.**\n",
    "\n",
    "That said, the result for K=100 is not entirely bad. After all, it gives us separate clusters for such categories as Scotland, Brazil, LGBT, computer science and the Mormon Church. If we set K somewhere between 25 and 100, we should be able to avoid breaking up clusters while discovering new ones.\n",
    "\n",
    "Also, we should ask ourselves how much **granularity** we want in our clustering. If we wanted a rough sketch of Wikipedia, we don't want too detailed clusters. On the other hand, having many clusters can be valuable when we are zooming into a certain part of Wikipedia.\n",
    "\n",
    "**There is no golden rule for choosing K. It all depends on the particular application and domain we are in.**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A7_k_means_with_text_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
