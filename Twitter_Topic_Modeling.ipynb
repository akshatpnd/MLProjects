{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MEd_2KEiQAeH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxEU6SuQxQHA"
   },
   "source": [
    "---\n",
    "### Data Loading\n",
    "\n",
    "First let's read the dataset into a data frame and have a look what is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0BUuFxxNr0-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,1,2,4,5,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>is_quote</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>country_code</th>\n",
       "      <th>place_full_name</th>\n",
       "      <th>place_type</th>\n",
       "      <th>verified</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asegura sus beneficios, registra a tu esposa e...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#COVID19 | El Faro conversó con policías, un f...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Si ya era cuestionable la burocracia, lo es má...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Las medidas de higiene ayudan a reducir la pro...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cubre tu nariz y boca al estornudar con el áng...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text is_quote is_retweet  \\\n",
       "0  Asegura sus beneficios, registra a tu esposa e...    False      False   \n",
       "1  #COVID19 | El Faro conversó con policías, un f...    False      False   \n",
       "2  Si ya era cuestionable la burocracia, lo es má...    False      False   \n",
       "3  Las medidas de higiene ayudan a reducir la pro...    False      False   \n",
       "4  Cubre tu nariz y boca al estornudar con el áng...    False      False   \n",
       "\n",
       "   retweet_count country_code place_full_name place_type verified lang  \n",
       "0            0.0          NaN             NaN        NaN    False   es  \n",
       "1           11.0          NaN             NaN        NaN     True   es  \n",
       "2            1.0          NaN             NaN        NaN    False   es  \n",
       "3           38.0          NaN             NaN        NaN     True   es  \n",
       "4            0.0          NaN             NaN        NaN    False   es  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/valentina-s/cse-stat-416-sp20/master/data/2020-04-30_Coronavirus_Tweets_small.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WuJ3-u1AzEpP"
   },
   "source": [
    "---\n",
    "### Text Preprocessing\n",
    "\n",
    "First, we will do several text preprocessing steps. We will:\n",
    "* limit to English language\n",
    "* remove URL links\n",
    "* make lower case\n",
    "* remove pronunciation\n",
    "* remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRkVrmV5OMqC"
   },
   "outputs": [],
   "source": [
    "# select tweets in English\n",
    "text_en = data['text'][data['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJSg5kFwPqiz"
   },
   "outputs": [],
   "source": [
    "# remove URL links\n",
    "text_en_lr = text_en.apply(lambda x: re.sub(r\"https\\S+\", \"\", str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TTCnKhC7P5Yc"
   },
   "outputs": [],
   "source": [
    "# make lower case\n",
    "text_en_lr_lc = text_en_lr.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7mT06_HP_rI"
   },
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "text_en_lr_lc_pr = text_en_lr_lc.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2KfQmuFQvug"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akshat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7v88F1frQaWR"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19','covid-19', 'covidー19'])\n",
    "\n",
    "text_en_lr_lc_pr_sr = text_en_lr_lc_pr.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OEcQyxPOz7zO"
   },
   "source": [
    "### TF-IDF Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90S6sBJcQo4-"
   },
   "outputs": [],
   "source": [
    "# create TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_df=0.95)  # ignore words with very high doc frequency\n",
    "tf_idf = vectorizer.fit_transform(text_en_lr_lc_pr_sr)\n",
    "\n",
    "# exctract also the words so that we know which feature corresponds to which word\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3m8NLFFwUf3i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198579, 255511)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u58C3bG-q4PQ"
   },
   "source": [
    "### Non-negative Matrix Decomposition for Topic Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TlfA2U29ONr"
   },
   "source": [
    "Next we will use the [NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) method from `scikit-learn` to extract the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyej3WnDUk-7"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pipa6j9FDfXJ"
   },
   "source": [
    "Set up an NMF model with 5 components. So that we all get the same results, please pass these parameters `init = 'nndsvd'` and `random_state = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_LuG5J6XKsP"
   },
   "outputs": [],
   "source": [
    "# define an NMF model with `n_components = 5`\n",
    "nmf = NMF(n_components=5, init='nndsvd', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3CoPzi9XPVe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0,\n",
       "  max_iter=200, n_components=5, random_state=1, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.fit(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niEspTjRXSkW"
   },
   "source": [
    "The topics are stored within the object `nmf.components_`. Now you can find the weight of each word within a topic. It will be interesting to look at the words corresponding to each topic ordered by their heighest weight. Remember the words corresponding to each topic are stored in `feature_names`, while the weights are stored in `nmf.components_`. You can use the [`argsort()`](https://numpy.org/doc/stable/reference/generated/numpy.argsort.html) function to extract the indeces of the sorted words. Note that `argsort` sorts from lowest to heighest so you need to look at the last values for the ones with heighest weights. You can reverse a list/array with `[::-1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rl3pz-D8YgzF"
   },
   "source": [
    "Find the maximum weight of a word in the first topic, and the word which corresponds to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P02QUBnmZ-BS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Topic:\n",
      "2.1784190593311634\n",
      "174646\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "HZ = nmf.components_\n",
    "print(\"First Topic:\")\n",
    "# max weight in first topic\n",
    "print(HZ[0,HZ[0,:].argsort()[-1]])\n",
    "# index of max weight and the word associated with it\n",
    "print(HZ[0,:].argsort()[-1])\n",
    "print(feature_names[174646])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXShnIs7Yex6"
   },
   "source": [
    "Create a function `words_from_topic` to extract an ordered list of words in a topic (highest weight first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVDXmfvEnSy7"
   },
   "outputs": [],
   "source": [
    "def words_from_topic(topic, feature_names):\n",
    "    ordered_words = []\n",
    "    arr = topic[1:].argsort()[::-1]\n",
    "    for ind in arr:\n",
    "        ordered_words.append(feature_names[ind])\n",
    "    return(ordered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHoAl8gSX8nF"
   },
   "outputs": [],
   "source": [
    "def print_top_words(components, feature_names, n_top_words):\n",
    "    \"\"\" \n",
    "    print_top_words prints the first n_top_words for each topic in components\n",
    "    \"\"\"\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        ordered_words = words_from_topic(topic, feature_names)\n",
    "        message = \"Topic #%d: \" % (topic_idx+1)\n",
    "        message += \", \".join(ordered_words[:n_top_words])\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wL2y0wFuYN6C",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: peopl, lockdowm, gesund, homdbs, stavtjoumakaris, likable, onduos, timdraper, knottycommander, daxx98\n",
      "Topic #2: casero, nevsinmengu, deathresulting, tot, confirmatory, reportdomesticabuse, numbencore, positions, reportout, todate\n",
      "Topic #3: helord, sprea, apoyototalalgobiernodemexico, selfreported, symptomquestion, downlights, sooncoronavirus, identifiying, slovensko, dailing\n",
      "Topic #4: uryt, chin, joielovesjax, trumbull, lesvos, druditraj, sayrealdonaldtrump, milling, realdonaldt, deathresulting\n",
      "Topic #5: pandemiamundial, healtcareheroes, suppor, helord, crisilresearch, workerrelief, gloating, newroselandhosp, reacts, responsable\n"
     ]
    }
   ],
   "source": [
    "print_top_words(nmf.components_, feature_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAwds09LkXC8"
   },
   "source": [
    "Next let's look at a specific tweet and the individual contributions of the topics. For that we need to look at the coordinates of the transformed original tf-idf features. That can be obtained through `nmf.fit_transform` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3XKtE20lPEJg"
   },
   "outputs": [],
   "source": [
    "tweets_projected = nmf.fit_transform(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBPHk2c7kmFT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198579, 5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_projected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kV0M40RgIuh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'attention seattle shoppers grocery stores working hard keep employees customers safe part help slow spread ☑️ limit trips ☑️ respect special shopping hours ☑️ follow socialdistance guidance stores wegotthisseattle'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_en_lr_lc_pr_sr.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxRUu0UWhAsb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025972237275222576"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_projected[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQ0ckLBiZTIU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft sees digital reboot pandemic profits'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_en_lr_lc_pr_sr.iloc[1]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "A8_Twitter_Topic_Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
